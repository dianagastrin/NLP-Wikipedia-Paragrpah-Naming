{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/utils/fixes.py:64: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n",
      "  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "import pickle,sys,os,lda,scipy,pandas\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "bios = []\n",
    "for suf in ['18000']:\n",
    "    bios += pandas.read_pickle(\"train-corpus/corpus\"+suf+\".pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(i, end_val, bar_length=50):\n",
    "    percent = float(i) / end_val\n",
    "    hashes = '#' * int(round(percent * bar_length))\n",
    "    spaces = ' ' * (bar_length - len(hashes))\n",
    "    sys.stdout.write(\"\\r{0} / {1} Percent: [{2}] {3}%\".format(i, end_val, hashes + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Samuel Cornish',\n",
       "  [('Summary',\n",
       "    \"Samuel Eli Cornish (1795  6 November 1858) was an American Presbyterian minister, abolitionist, publisher, and journalist.  He was a leader in New York City's small free black community, where he organized the first congregation of black Presbyterians in New York. In 1827 he became one of two editors of the newly founded ''Freedom's Journal'', the first black newspaper in the United States. In 1833 he was a founding member of the interracial American Anti-Slavery Society.\\n\\n\"),\n",
       "   ('Career',\n",
       "    \"\\nWhen Cornish was ordained in 1822, his parish was officially established as the New Demeter Street Presbyterian Church, making it the first black Presbyterian Church in New York City. He later ministered at the First African Presbyterian Church in Philadelphia, and Emmanuel Church in New York City. Cornish held high-ranking positions within the American Bible Society and the American Missionary Association, founded in 1846. He was one of the four founding black members; there were a total of 12 founders.\\n\\nIn March 1827 he became one of two editors of ''Freedom's Journal'', the first black newspaper in the United States. The other editor was John Russwurm. It was intended to serve the 300,000 free blacks in the country and especially New York's community, as well as to offset the racist commentary of local papers in the city. Cornish left the paper in September 1827, returning two years later.\\n\\nDuring this time, Russwurm had advocated colonization in Africa for free American blacks, and lost many readers. He emigrated to Liberia in 1829. Cornish returned to the paper and tried to revive it, changing the name to ''The Rights of All,'' but the paper folded in less than a year. Cornish later was editor for ''Colored American'' from 1837 to 1839.\\n\\nIn 1833 Cornish was one of the founding members of the American Anti-Slavery Society, whose membership and leaders were interracial. He was active with them until 1840. That year, he left to join the newly formed American and Foreign Anti-Slavery Society, largely because of disputes with William Lloyd Garrison over religion in the Abolitionist movement. Cornish used his position as a journalist and editor to inform the public on the issues involving abolitionism.\\n\\n\")])]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find noise words (in order to filter them later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 / 1000 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "tokens_freqs = dict()\n",
    "\n",
    "i = 0\n",
    "for bio in bios:\n",
    "    for segment in bio[0][1]:\n",
    "        \n",
    "        for paragraph_text in segment[1].split('\\n'):\n",
    "            tokens = word_tokenize(paragraph_text)\n",
    "            for token in tokens:\n",
    "                if token in tokens_freqs:\n",
    "                    tokens_freqs[token] += 1\n",
    "                else:\n",
    "                    tokens_freqs[token] = 1\n",
    "    i += 1\n",
    "    progress(i, len(bios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "# Sorder tokens by frequency in reverse order (most frequent first):\n",
    "most_frequent_tokens = sorted(tokens_freqs.items(), key=operator.itemgetter(1))\n",
    "most_frequent_tokens.reverse()\n",
    "\n",
    "noise_words = set()\n",
    "for token_and_freq in most_frequent_tokens:\n",
    "    # We take the threshold to be 1/100 the frequency of the most frequnent token:\n",
    "    if token_and_freq[1] > most_frequent_tokens[0][1] / 100:\n",
    "        noise_words.add(token_and_freq[0])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect paragraph data for every biography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 / 1000 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "bios_df = list()\n",
    "all_tokens = set()\n",
    "i = 0\n",
    "for bio in bios:\n",
    "    data = {\n",
    "        'person' : bio[0][0],\n",
    "        'tokenized_paragraphs' : list(),\n",
    "        'paragraph_splits' : list(),\n",
    "        'word_splits': [0],\n",
    "        'length' : 0,\n",
    "        'segments' : 0\n",
    "    }\n",
    "    number_of_words = 0\n",
    "    for segment in bio[0][1]:\n",
    "        number_of_paragraphs = 0\n",
    "        data['segments'] += 1\n",
    "        for paragraph_text in segment[1].split('\\n'):\n",
    "            original_tokens = word_tokenize(paragraph_text)\n",
    "            tokens = list()\n",
    "            for token in original_tokens:\n",
    "                if token not in noise_words:\n",
    "                    tokens.append(token)\n",
    "            \n",
    "            if len(tokens) > 0:\n",
    "                number_of_paragraphs += 1\n",
    "                number_of_words += len(tokens)\n",
    "                all_tokens |= set(tokens)\n",
    "                data['tokenized_paragraphs'].append(tokens)  \n",
    "        data['paragraph_splits'].append(number_of_paragraphs)\n",
    "        data['word_splits'].append(number_of_words)\n",
    "    data['length'] = number_of_words\n",
    "    bios_df.append(data)\n",
    "    i += 1\n",
    "    progress(i, len(bios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>paragraph_splits</th>\n",
       "      <th>person</th>\n",
       "      <th>segments</th>\n",
       "      <th>tokenized_paragraphs</th>\n",
       "      <th>word_splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176</td>\n",
       "      <td>[1, 4]</td>\n",
       "      <td>Samuel Cornish</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Samuel, Eli, Cornish, 1795, 6, 1858, Presbyt...</td>\n",
       "      <td>[0, 34, 176]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>John Renshaw Starr</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Renshaw, Starr, 1996, sons, Alfred, Demarest...</td>\n",
       "      <td>[0, 25, 73]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length paragraph_splits              person  segments  \\\n",
       "0     176           [1, 4]      Samuel Cornish         2   \n",
       "1      73           [1, 2]  John Renshaw Starr         2   \n",
       "\n",
       "                                tokenized_paragraphs   word_splits  \n",
       "0  [[Samuel, Eli, Cornish, 1795, 6, 1858, Presbyt...  [0, 34, 176]  \n",
       "1  [[Renshaw, Starr, 1996, sons, Alfred, Demarest...   [0, 25, 73]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios_data = pandas.DataFrame(bios_df)\n",
    "bios_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10272 / 10272 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "all_tokens_list = list(all_tokens)\n",
    "number_of_tokens = len(all_tokens_list)\n",
    "all_paragraphs = bios_data['tokenized_paragraphs'].sum()\n",
    "paragraphs_bow = np.zeros([len(all_paragraphs),number_of_tokens], dtype = np.int)\n",
    "tokens_indices_dict = dict()\n",
    "for i in range(number_of_tokens):\n",
    "    tokens_indices_dict[all_tokens_list[i]] = i\n",
    "    \n",
    "for i in range(len(all_paragraphs)):\n",
    "    for w in all_paragraphs[i]:\n",
    "        paragraphs_bow[i][tokens_indices_dict[w]] += 1\n",
    "        \n",
    "    progress(i + 1, len(all_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign LDA topics to paragraphs with word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate topic lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using gensim lda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(all_paragraphs)\n",
    "corpus = [dictionary.doc2bow(text) for text in all_paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "lda = LdaModel(corpus, id2word=dictionary, num_topics=20, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: life political both said there than some so did This out them any world could\n",
      "Topic 2: 2009 DJ File computer Sciences Rudd former computing Angels Minister Most Prime George ''Adventure International\n",
      "Topic 3: Mercier Quebec named *The engine Canada According memory Dayton power government effort being case Chinese\n",
      "Topic 4: worked Berlin several game band moved Company California original Frank Island home label recorded performed\n",
      "Topic 5: California Art elected U.S. Senate Party losing active election former House Washington Republican both leader\n",
      "Topic 6: Princeton 36 67 Duke Society der artists Award No Poland player Royal ** Symphony Reformed\n",
      "Topic 7: Robert Life wife King title father instruments starred author 1956 Mihailovic By musical William George\n",
      "Topic 8: Open Award 2014 46 2012 2010 2011 2008 64 Australian Best won 2006 26 ISBN\n",
      "Topic 9: Music CD television played role career playing appeared Hurley 1995 directed marriage Love novel character\n",
      "Topic 10: Lloyd Hall B Wright ''Disko father 62 mother Fame ! From began parents jazz wife\n",
      "Topic 11: Panofsky League art major Major Texas Masters played career studied Baseball 1989 home study America\n",
      "Topic 12: Batlle theory Frazetta artist Soviet president collection — mathematical field own North public new highly\n",
      "Topic 13: College Academy National received Institute appointed Arts Royal Order School British Cambridge Association flight bill\n",
      "Topic 14: October February July politician 2 April 5 4 22 3 10 12 7 ''International 15\n",
      "Topic 15: round Orville 63 Army 57 % command Mosby men lost no At Opera military Western\n",
      "Topic 16: composer Orchestra Paris Louis studied Montreal McPhee conductor French Marie part Bali Raymond France Henry\n",
      "Topic 17: Taylor Records Arminius Gigolo Deejay Miller church Soviet Church hall Missouri Wilbur City 1919 Black\n",
      "Topic 18: Hell Nelson named Scott Russian Young school culture extensively News Sherman Cemetery recorded father age\n",
      "Topic 19: album Lady Music Hemmings daughter Los David Angeles Other 1964 1975 producer 2 Festival 1967\n",
      "Topic 20: early piano works performances use composition performed concert such Irish / developed based opera late\n"
     ]
    }
   ],
   "source": [
    "topics_words = list()\n",
    "number_of_topics = 20\n",
    "\n",
    "i = 0\n",
    "for topic in lda.show_topics(num_topics=number_of_topics, formatted=False, num_words=len(all_tokens)):\n",
    "    i += 1\n",
    "    topic_words = list()\n",
    "    for score, word in topic:\n",
    "        topic_words.append(word)\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words[:15])))\n",
    "    topics_words.append(topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_words_indices_dicts = list()\n",
    "for topic_words in topics_words:\n",
    "    topic_words_indices_dict = dict()\n",
    "    for i in range(len(topic_words)):\n",
    "        topic_words_indices_dict[topic_words[i]] = i\n",
    "    topic_words_indices_dicts.append(topic_words_indices_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# topic_words_indices_dicts[2]['Samuel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_word_topic_id(word, topics_words):\n",
    "    topic_id = -1\n",
    "    min_topic_index = number_of_tokens + 1\n",
    "    for current_topic_id in range(number_of_topics):\n",
    "        index = topic_words_indices_dicts[current_topic_id][word]\n",
    "        if index < min_topic_index:\n",
    "            topic_id = current_topic_id\n",
    "            min_topic_index = index\n",
    "            \n",
    "    return topic_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_vocabulary_topic_ids(vocabulary, topics_words):\n",
    "    topic_ids = list()\n",
    "    for word in vocab:\n",
    "        topic_ids.append(calculate_word_topic_id(word, topics_words))\n",
    "        \n",
    "    return topic_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = all_tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary_topic_ids = calculate_vocabulary_topic_ids(vocab, topics_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocabulary_word_topic_id(vocabulary, vocabulary_topic_ids, word):\n",
    "    if word in vocabulary:\n",
    "        return vocabulary_topic_ids[vocabulary.index(word)]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vocabulary_word_topic_id(vocab, vocabulary_topic_ids, \"Samuel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentence_topic_ids(sentence, topics_words):\n",
    "    topic_ids = list()\n",
    "    for word in sentence:\n",
    "        topic_id = get_vocabulary_word_topic_id(vocab, vocabulary_topic_ids, word)\n",
    "        if topic_id is not None:\n",
    "            topic_ids.append(topic_id)\n",
    "        \n",
    "    return topic_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sentences_topic_ids(sentences, topic_words):\n",
    "    topic_ids = list()\n",
    "    for sentence in sentences:\n",
    "        topic_ids.append(get_sentence_topic_ids(sentence, topics_words))\n",
    "        \n",
    "    return topic_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_biography_sentences_topic_ids(biography_sentences_tokens, topic_words):\n",
    "    biography_sentences_topic_ids = list()\n",
    "    for sentences in biography_sentences_tokens:\n",
    "        biography_sentences_topic_ids.append(get_sentences_topic_ids(sentences, topic_words))\n",
    "    \n",
    "    return biography_sentences_topic_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_sentence_topics_frequency(sentence_topic_ids, number_of_topics):\n",
    "    topics_frequencies = np.zeros(number_of_topics)\n",
    "    for topic_id in sentence_topic_ids:\n",
    "        topics_frequencies[topic_id] += 1\n",
    "    \n",
    "    return topics_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_sentences_topics_frequency(sentences_topic_ids, number_of_topics):\n",
    "    sentences_topics_frequencies = list()\n",
    "    for sentence in sentences_topic_ids:\n",
    "        sentences_topics_frequencies.append(calculate_sentence_topics_frequency(sentence, number_of_topics))\n",
    "        \n",
    "    return sentences_topics_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_biography_sentences_topics_frequencies(biography_sentences_topic_ids, number_of_topics):\n",
    "    biography_sentences_topics_frequencies = list()\n",
    "    for sentences in biography_sentences_topic_ids:\n",
    "        biography_sentences_topics_frequencies.append(calculate_sentences_topics_frequency(sentences, number_of_topics))\n",
    "        \n",
    "    return biography_sentences_topics_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_coherence_score(sentences_topics_frequencies, number_of_topics, position, window_size):\n",
    "    pre_topics_frequencies = np.zeros(number_of_topics)\n",
    "    for i in range(window_size):\n",
    "        pre_topics_frequencies += sentences_topics_frequencies[position - i]\n",
    "        \n",
    "    post_topics_frequencies = np.zeros(number_of_topics)\n",
    "    for i in range(window_size):\n",
    "        post_topics_frequencies += sentences_topics_frequencies[position + 1 + i]\n",
    "        \n",
    "    dot_product = np.dot(pre_topics_frequencies, post_topics_frequencies)\n",
    "    pre_norm = np.linalg.norm(pre_topics_frequencies)\n",
    "    post_norm = np.linalg.norm(post_topics_frequencies)\n",
    "    \n",
    "    cosine_similarity = dot_product / (pre_norm * post_norm)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_coherence_scores(sentences_topics_frequencies, number_of_topics, window_size):\n",
    "    coherence_scores = list()\n",
    "    for i in range(len(sentences_topics_frequencies) - 2 * window_size):\n",
    "        score = calculate_coherence_score(sentences_topics_frequencies, number_of_topics, window_size + i - 1, window_size)\n",
    "        coherence_scores.append(score)\n",
    "        \n",
    "    return coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_depth_scores(coherence_scores):\n",
    "    depth_scores = list()\n",
    "    for i in range(len(coherence_scores)):\n",
    "        hl = coherence_scores[i]\n",
    "        hr = coherence_scores[i]\n",
    "        for j in range(i):\n",
    "            if coherence_scores[i - j] > coherence_scores[i]:\n",
    "                hl = coherence_scores[i - j]\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        for j in range(i + 1, len(coherence_scores) - i):\n",
    "            if coherence_scores[j] > coherence_scores[i]:\n",
    "                hr = coherence_scores[j]\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        depth_score = 0.5 * (hl + hr - 2 * coherence_scores[i])\n",
    "        depth_scores.append(depth_score)\n",
    "        \n",
    "    return depth_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "biography_sentences_topic_ids = get_biography_sentences_topic_ids(bios_df[1]['tokenized_paragraphs'], topic_words)\n",
    "biography_sentences_topics_frequencies = calculate_biography_sentences_topics_frequencies(biography_sentences_topic_ids, number_of_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Renshaw',\n",
       "  'Starr',\n",
       "  '1996',\n",
       "  'sons',\n",
       "  'Alfred',\n",
       "  'Demarest',\n",
       "  'Starr',\n",
       "  'Ethel',\n",
       "  'Renshaw',\n",
       "  'English',\n",
       "  'grandson',\n",
       "  'William',\n",
       "  'Robert',\n",
       "  'Renshaw',\n",
       "  'artist',\n",
       "  'soldier',\n",
       "  'Second',\n",
       "  'story',\n",
       "  'told',\n",
       "  'book',\n",
       "  'Starr',\n",
       "  'Affair',\n",
       "  'Jean',\n",
       "  'Overton',\n",
       "  'Fuller'],\n",
       " ['By',\n",
       "  'exploiting',\n",
       "  'ability',\n",
       "  'pass',\n",
       "  'himself',\n",
       "  'off',\n",
       "  'Frenchman',\n",
       "  'joined',\n",
       "  'group',\n",
       "  'French',\n",
       "  'Belgian',\n",
       "  'prisoners',\n",
       "  'released',\n",
       "  'custody',\n",
       "  'Red',\n",
       "  'Cross',\n",
       "  'taken',\n",
       "  'Switzerland',\n",
       "  'war',\n",
       "  'Europe',\n",
       "  'drew',\n",
       "  'close'],\n",
       " ['Stories',\n",
       "  'SOE',\n",
       "  'agents',\n",
       "  'shared',\n",
       "  'captivity',\n",
       "  'Avenue',\n",
       "  'Foch',\n",
       "  'resulted',\n",
       "  'doubts',\n",
       "  'being',\n",
       "  'raised',\n",
       "  'loyalty',\n",
       "  'case',\n",
       "  'subject',\n",
       "  'MI5',\n",
       "  'investigation',\n",
       "  'concluded',\n",
       "  'although',\n",
       "  'behaviour',\n",
       "  'certainly',\n",
       "  'suspicious',\n",
       "  'there',\n",
       "  'no',\n",
       "  'grounds',\n",
       "  'criminal',\n",
       "  'prosecution']]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios_df[1]['tokenized_paragraphs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coherence_scores = calculate_coherence_scores(biography_sentences_topics_frequencies[0], number_of_topics, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "depth_scores = calculate_depth_scores(coherence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean = np.mean(depth_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std = np.std(depth_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0029486130296959706"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean - std /2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.11815613696359856,\n",
       " 0.010876058127594956,\n",
       " 0.0,\n",
       " 0.1228966707500575,\n",
       " 0.0017059410337709746,\n",
       " 0.0,\n",
       " 0.045401051710649343,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John Renshaw Starr',\n",
       "  [('Summary',\n",
       "    \"John Renshaw Starr (died 1996), was one of two sons of Alfred Demarest Starr (an American) and Ethel Renshaw (English). He was a grandson of William Robert Renshaw. He was an artist and a soldier during the Second World War. His story is told in a book, ''The Starr Affair'', by Jean Overton Fuller.\\n\\n\"),\n",
       "   ('Release',\n",
       "    'By exploiting his ability to pass himself off as a Frenchman, he joined a group of French and Belgian prisoners who were released into the custody of the Red Cross and taken to Switzerland as the war in Europe drew to a close.\\n\\nStories from other SOE agents who shared his captivity at the Avenue Foch resulted in doubts being raised about his loyalty, and his case became the subject of an MI5 investigation, which concluded that although his behaviour was certainly suspicious, there were no grounds for criminal prosecution. \\n\\n')])]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(biography_sentences_topics_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
