{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle,sys,os,lda,scipy,pandas,operator\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "bios = []\n",
    "for suf in ['2000', '4000', '6000', '8000','10000' ,'12000']:\n",
    "    bios += pandas.read_pickle(\"train-corpus/corpus\"+suf+\".pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(i, end_val, bar_length=50):\n",
    "    percent = float(i) / end_val\n",
    "    hashes = '#' * int(round(percent * bar_length))\n",
    "    spaces = ' ' * (bar_length - len(hashes))\n",
    "    sys.stdout.write(\"\\r{0} / {1} Percent: [{2}] {3}%\".format(i, end_val, hashes + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jean-Pierre Abbat',\n",
       "  [('Biography',\n",
       "    'Jean-Pierre Fernand Noel Abbat (June 17, 1928  August 1, 1993) was, with Dr. Fritz Hartmann, the first person in the USA to manufacture polyurethane.\\n\\nAbbat was born in Le Trait, Normandy to a shipbuilder. He met Marina Larde at the Sorbonne and they were married, migrating to the United States in 1953.\\nIn 1962 Abbat proposed to Norman McCulloch to make a ballistically equivalent bowling pin out of polyurethane foam. Bowling pins were then made out of wood, with two cylindrical voids, and covered with a thin coating. The polyurethane pin would last much longer than the wooden pin date=August 2007. The American Bowling Congress were against the idea because it would put Brunswick and AMF, the biggest bowling pin makers, out of business date=August 2007.\\n\\nAbbat worked for U-Do, Mattel, Kenner, Fisher-Price, and ITT, making toys and telephone parts out of urethane and other plastics.\\n\\nAbbat died in Raleigh, North Carolina of colon cancer. He is survived by his wife Marina Abbat, his daughter Kate Threefoot, and his son Pierre Abbat.\\n\\nDEFAULTSORT:Abbat, Jean-Pierre\\nC births\\nC deaths\\nCategory:University of Paris alumni\\nCategory:French emigrants to the United States\\nCategory:Deaths from colorectal cancer\\nCategory:Cancer deaths in North Carolina\\nCategory:French chemists\\n\\n\\nfrance-chemist-stub\\n49h40jv5vdyucve3bupy8241mrrc9sd\\n\\n\\n')])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find noise words (in order to filter them later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 / 6000 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "tokens_freqs = dict()\n",
    "i = 0\n",
    "for bio in bios:\n",
    "    for segment in bio[0][1]:\n",
    "        for paragraph_text in segment[1].split('\\n'):\n",
    "            tokens = word_tokenize(paragraph_text)\n",
    "            for token in tokens:\n",
    "                if token in tokens_freqs:\n",
    "                    tokens_freqs[token] += 1\n",
    "                else:\n",
    "                    tokens_freqs[token] = 1\n",
    "    i += 1\n",
    "    progress(i, len(bios))\n",
    "# Sorder tokens by frequency in reverse order (most frequent first):\n",
    "most_frequent_tokens = sorted(tokens_freqs.items(), key=operator.itemgetter(1))\n",
    "most_frequent_tokens.reverse()\n",
    "\n",
    "noise_words = set()\n",
    "for token_and_freq in most_frequent_tokens:\n",
    "    # We take the threshold to be 1/100 the frequency of the most frequnent token:\n",
    "    if token_and_freq[1] > most_frequent_tokens[0][1] / 100:\n",
    "        noise_words.add(token_and_freq[0])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect paragraph data for every biography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 / 6000 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "bios_df = list()\n",
    "all_tokens = set()\n",
    "i = 0\n",
    "for bio in bios:\n",
    "    data = {\n",
    "        'person' : bio[0][0],\n",
    "        'tokenized_paragraphs' : list(),\n",
    "        'paragraph_splits' : list(),\n",
    "        'word_splits': [0],\n",
    "        'length' : 0,\n",
    "        'segments' : 0\n",
    "    }\n",
    "    number_of_words = 0\n",
    "    for segment in bio[0][1]:\n",
    "        number_of_paragraphs = 0\n",
    "        data['segments'] += 1\n",
    "        for paragraph_text in segment[1].split('\\n'):\n",
    "            original_tokens = word_tokenize(paragraph_text)\n",
    "            tokens = list()\n",
    "            for token in original_tokens:\n",
    "                if token not in noise_words:\n",
    "                    tokens.append(token)\n",
    "            \n",
    "            if len(tokens) > 0:\n",
    "                number_of_paragraphs += 1\n",
    "                number_of_words += len(tokens)\n",
    "                all_tokens |= set(tokens)\n",
    "                data['tokenized_paragraphs'].append(tokens)  \n",
    "        data['paragraph_splits'].append(number_of_paragraphs)\n",
    "        data['word_splits'].append(number_of_words)\n",
    "    data['length'] = number_of_words\n",
    "    bios_df.append(data)\n",
    "    i += 1\n",
    "    progress(i, len(bios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>paragraph_splits</th>\n",
       "      <th>person</th>\n",
       "      <th>segments</th>\n",
       "      <th>tokenized_paragraphs</th>\n",
       "      <th>word_splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>Leonard Rossiter</td>\n",
       "      <td>3</td>\n",
       "      <td>[[Leonard, Rossiter, 21, October, 1926, –, 5, ...</td>\n",
       "      <td>[0, 37, 77, 99]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137</td>\n",
       "      <td>[15]</td>\n",
       "      <td>Jean-Pierre Abbat</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Jean-Pierre, Fernand, Noel, Abbat, June, 17,...</td>\n",
       "      <td>[0, 137]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length paragraph_splits             person  segments  \\\n",
       "0      99        [1, 1, 1]   Leonard Rossiter         3   \n",
       "1     137             [15]  Jean-Pierre Abbat         1   \n",
       "\n",
       "                                tokenized_paragraphs      word_splits  \n",
       "0  [[Leonard, Rossiter, 21, October, 1926, –, 5, ...  [0, 37, 77, 99]  \n",
       "1  [[Jean-Pierre, Fernand, Noel, Abbat, June, 17,...         [0, 137]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(all_tokens)\n",
    "bios_data = pandas.DataFrame(bios_df)\n",
    "bios_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_tokens_list = list(all_tokens)\n",
    "number_of_tokens = len(all_tokens_list)\n",
    "all_paragraphs = bios_data['tokenized_paragraphs'].sum()\n",
    "# print(len(all_paragraphs))\n",
    "# paragraphs_bow = np.zeros([len(all_paragraphs),number_of_tokens], dtype = np.int)\n",
    "# tokens_indices_dict = dict()\n",
    "# for i in range(number_of_tokens):\n",
    "#     tokens_indices_dict[all_tokens_list[i]] = i\n",
    "    \n",
    "# for i in range(len(all_paragraphs)):\n",
    "#     for w in all_paragraphs[i]:\n",
    "#         paragraphs_bow[i][tokens_indices_dict[w]] += 1\n",
    "        \n",
    "#     progress(i + 1, len(all_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign LDA topics to paragraphs with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaMulticore\n",
    "dictionary = corpora.Dictionary(all_paragraphs)\n",
    "corpus = [dictionary.doc2bow(text) for text in all_paragraphs]\n",
    "#lda = LdaMulticore(corpus,workers=3,id2word=dictionary, num_topics=20, passes=20)\n",
    "#with open('genlda6000.pkl','wb') as f:\n",
    "#    pickle.dump(lda,f)\n",
    "lda = pandas.read_pickle('genlda6000.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using gensim lda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecs = pickle.load(open('/home/ilay/vecs.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.show_topics(num_topics=5, formatted=False, num_words=5)\n",
    "#sum([w[1] for w in t[1][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Our Vectors Method with a Simple LDA based Paragraph Topic Score\n",
    "Instead of adding up word vectors we can just sum up the probability that a word is in the given topic for all the paragraph words. To get the same list of topic probabilities per paragraph that we do with the vectors.\n",
    "For that, though, we have to pass over the vocabulary and make a *number of topics* sized list for every word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate topic lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics_words = list()\n",
    "\n",
    "# word_freqs = dict()\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    # topic_words: words sorted by relevance to topic in descending order\n",
    "    topic_words = list(np.array(vocab)[np.argsort(topic_dist)[::-1]])#[:10]#[:-(n_top_words+1):-1]\n",
    "    #print(topic_words)\n",
    "    topics_words.append(topic_words)\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words[:15])))\n",
    "#     for word in topic_words[:20]:\n",
    "#         if word not in word_freqs:\n",
    "#             word_freqs[word] = 1\n",
    "#         else:\n",
    "#             word_freqs[word] += 1\n",
    "\n",
    "#word_topic_dists = dict()\n",
    "#for j,word in enumerate(vocab[:10]):\n",
    "#    for topic in range(lda.num_topics):\n",
    "#        topiclist = lda.show_topic(topic,len(all_tokens))\n",
    "#        print(topiclist[:5],topiclist[-5:])\n",
    "#        i = 0\n",
    "#        notfound = True\n",
    "#        word_topic_dists[word] = list()\n",
    "#         while notfound and i < len(all_tokens):\n",
    "#             if topiclist[i][0] == word:\n",
    "#                 word_topic_dists[word].append(topiclist[i][1])\n",
    "#                 noutfound = False\n",
    "#            i += 1\n",
    "#    progress(j,10)\n",
    "#word_topic_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: King II Duke married daughter Prince Henry Queen king Roman III Emperor Charles wife brother\n",
      "Topic 1: French Paris France la des Louis et du La Jean Danish ''Le War Battle Marie\n",
      "Topic 2: School College United school States William professor studied family British named worked appointed England moved\n",
      "Topic 3: out against did than them said so no could This up being himself people over\n",
      "Topic 4: Order der Cross und Knight Grand vols. Cavendish Berlin des History die Letters clan edited\n",
      "Topic 5: ISBN Press & ** Books ed Poems Toronto ''A Other 2 Time Stories 1989 edition\n",
      "Topic 6: family married wife children buried home near last daughter Cemetery They mother three house age\n",
      "Topic 7: Award Best won Prize Film received Awards nominated Academy Golden 1998 Festival 1993 1997 nomination\n",
      "Topic 8: World War won United career Army team age world League second season II States over\n",
      "Topic 9: television role appeared show played films TV actor BBC starred radio roles comedy character stage\n",
      "Topic 10: political Party government election Minister elected party President China member leader served state president Prime\n",
      "Topic 11: Category File sur Opera la Puerto del — Spain no et Fujiwara Rico & %\n",
      "Topic 12: ! ? You n't My Love Me 2007 Records UK / Little Brown Is 2006\n",
      "Topic 13: January March October July December May November September August June April February – 3 German\n",
      "Topic 14: theory research used Sun developed computer field development design named system systems method use mathematical\n",
      "Topic 15: book works books author English writer novel written novels writing short history stories fiction best\n",
      "Topic 16: album band music released song recorded songs & Records recording albums guitar group record rock\n",
      "Topic 17: works $ such music often million early style than Ford century period described these well\n",
      "Topic 18: Theatre 1985 director 2010 1988 1969 Powell Owen 1938 Princeton Orchestra De 1932 psychology 1955\n",
      "Topic 19: National Hall Museum Fame Art International Society Medal named awarded Arts Center Royal Academy Association\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "165255"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_words = list()\n",
    "word_topics = dict()\n",
    "for topic in lda.show_topics(num_topics=20, formatted=False, num_words=len(all_tokens)):\n",
    "    i += 1\n",
    "    topic_words = list()\n",
    "    for word,score in topic[1]:\n",
    "        topic_words.append(word)\n",
    "        if word in word_topics:\n",
    "            word_topics[word].append(score)\n",
    "        else:\n",
    "            word_topics[word] = [score]\n",
    "    print('Topic {}: {}'.format(topic[0], ' '.join(topic_words[:15])))\n",
    "    topics_words.append(topic_words)\n",
    "len(word_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 5\n",
      "ISBN : 0.0292459357528\n",
      "Press : 0.0150213496661\n",
      "& : 0.00996607470323\n",
      "** : 0.00900567612708\n",
      "Books : 0.00611599292641\n",
      "ed : 0.00513250684778\n",
      "Poems : 0.00455631778693\n",
      "Toronto : 0.00447018227349\n",
      "''A : 0.00409525258403\n",
      "Other : 0.00406324248051\n",
      "=====\n",
      "topic 6\n",
      "family : 0.00905247612984\n",
      "married : 0.00851100646704\n",
      "wife : 0.00773300219653\n",
      "children : 0.00746579073769\n",
      "buried : 0.00720795770982\n",
      "home : 0.00619619201968\n",
      "near : 0.00454598226231\n",
      "last : 0.00438021809429\n",
      "daughter : 0.00436837548787\n",
      "Cemetery : 0.00413003008392\n",
      "=====\n",
      "topic 7\n",
      "Award : 0.0289089655523\n",
      "Best : 0.01734625488\n",
      "won : 0.0116144870165\n",
      "Prize : 0.00855735068612\n",
      "Film : 0.00694703535124\n",
      "received : 0.00686637096292\n",
      "Awards : 0.00685934597184\n",
      "nominated : 0.00679549673394\n",
      "Academy : 0.00597625910701\n",
      "Golden : 0.0058161631191\n",
      "=====\n",
      "topic 8\n",
      "World : 0.0138170958184\n",
      "War : 0.0116821800404\n",
      "won : 0.00530050924546\n",
      "United : 0.00511337688015\n",
      "career : 0.00396996318163\n",
      "Army : 0.00387049807003\n",
      "team : 0.00381373633574\n",
      "age : 0.00369362230112\n",
      "world : 0.00356701996582\n",
      "League : 0.00323663857823\n",
      "=====\n",
      "topic 9\n",
      "television : 0.0110313181397\n",
      "role : 0.0108784420197\n",
      "appeared : 0.00971025262777\n",
      "show : 0.00897332187064\n",
      "played : 0.00841232476541\n",
      "films : 0.00766180593597\n",
      "TV : 0.00637755372736\n",
      "actor : 0.00534697349392\n",
      "BBC : 0.00515612125777\n",
      "starred : 0.004856379899\n",
      "=====\n",
      "topic 10\n",
      "political : 0.0106092572432\n",
      "Party : 0.0102509566157\n",
      "government : 0.00979061375771\n",
      "election : 0.00623475611126\n",
      "Minister : 0.00590743625389\n",
      "elected : 0.00566853446328\n",
      "party : 0.00562566899561\n",
      "President : 0.00562030644545\n",
      "China : 0.00472503182327\n",
      "member : 0.00446181942725\n",
      "=====\n",
      "topic 11\n",
      "Category : 0.00794312880266\n",
      "File : 0.00502058399766\n",
      "sur : 0.0040478946743\n",
      "Opera : 0.0039721817853\n",
      "la : 0.00373029739624\n",
      "Puerto : 0.00368363657849\n",
      "del : 0.003176914921\n",
      "— : 0.00289838926822\n",
      "Spain : 0.00283117208887\n",
      "no : 0.00266233092848\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for j in range(5,12):\n",
    "    print(\"topic\",j)\n",
    "    for i in range(10):\n",
    "        word = topics_words[j][i]\n",
    "        print(word,\":\",word_topics[word][j])\n",
    "    print(\"=====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paragraph_to_score(paragraph_tokens,topic_index):\n",
    "    return sum(word_topics[t][topic_index] for t in paragraph_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paragraph_to_vector(paragraph_tokens):\n",
    "    l = len(vecs['queen']) # len of the vector is 300\n",
    "    paragraph_accumulative = np.zeros(l)\n",
    "    topic_ratings = []\n",
    "    # just sum the paragraph words' vectors to get a semantic average of it\n",
    "    for ind,word in enumerate(paragraph_tokens):\n",
    "        if word in vecs:\n",
    "            paragraph_accumulative += vecs[word]\n",
    "    return paragraph_accumulative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each topic, make a representing vector by summing it's first 200 word-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_vectors = list()\n",
    "for topic_words in topics_words:\n",
    "    words_taken = 0\n",
    "    i = 0\n",
    "    vector = np.zeros(300)\n",
    "    while(words_taken < 200):\n",
    "        word = topic_words[i]\n",
    "#         if (word not in word_freqs or word_freqs[word] < 5) and word in vecs:\n",
    "        if word in vecs:\n",
    "            vector += vecs[word]\n",
    "            words_taken += 1\n",
    "        i += 1\n",
    "    topic_vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a list of topics for each paragraph by distance of topic vectors from the paragraph vector\n",
    "def paragraph_topics_rating_vecs(paragraph,topic_vectors):\n",
    "    cosine = scipy.spatial.distance.cosine\n",
    "    return np.argsort([cosine(paragraph_to_vector(paragraph),topic_vector) for i, topic_vector in enumerate(topic_vectors)])\n",
    "\n",
    "\n",
    "def paragraph_topics_rating_ldascores(paragraph):\n",
    "    return np.argsort([1/paragraph_to_score(paragraph,i) for i in range(lda.num_topics)])\n",
    "\n",
    "def paragraph_topics_rating(paragraph):\n",
    "    return np.argsort([sum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_paragraph = bios_data.loc[1,'tokenized_paragraphs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  1,  0, 18,  5,  8, 11,  2,  4,  7, 14,  6, 19, 12,  9, 16, 17,\n",
       "       10, 15,  3])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_topics_rating_vecs(example_paragraph,topic_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  8,  7,  5, 16,  0, 12, 19,  9, 18, 14,  6,  2,  3,  4, 15, 10,\n",
       "       11, 17,  1])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_topics_rating_ldascores(example_paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the text according to the topic ratings with and without vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 / 6000 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "psplits_v =  list()\n",
    "wsplits_v = list()\n",
    "psplits_s = list()\n",
    "wsplits_s = list()\n",
    "\n",
    "for i in range(len(bios_data)):\n",
    "    last_paragraph_topics_vecs = np.array([])\n",
    "    last_paragraph_topics_scores = np.array([])\n",
    "    number_of_paragraphs_vecs = number_of_paragraphs_scores = 1\n",
    "    number_of_words = 0\n",
    "    psplit_v = list()\n",
    "    psplit_s = list()\n",
    "    wsplit_v = list() \n",
    "    wsplit_s = list()\n",
    "    for tp in bios_data.loc[i,'tokenized_paragraphs']:\n",
    "        number_of_words += len(tp)\n",
    "        paragraph_topics_vecs = paragraph_topics_rating_vecs(tp,topic_vectors)[:3]\n",
    "        paragraph_topics_scores = paragraph_topics_rating_ldascores(tp)[:3]\n",
    "        if len(last_paragraph_topics_vecs) > 0:\n",
    "            if len(np.intersect1d(paragraph_topics_vecs, last_paragraph_topics_vecs)) > 0:\n",
    "                number_of_paragraphs_vecs += 1\n",
    "            else:\n",
    "                psplit_v.append(number_of_paragraphs_vecs)\n",
    "                wsplit_v.append(number_of_words)\n",
    "                number_of_paragraphs_vecs = 1\n",
    "        else:\n",
    "            wsplit_v.append(0)\n",
    "        if len(last_paragraph_topics_scores) > 0:\n",
    "            if len(np.intersect1d(paragraph_topics_scores,last_paragraph_topics_scores)) > 0:\n",
    "                number_of_paragraphs_scores += 1\n",
    "            else:\n",
    "                psplit_s.append(number_of_paragraphs_scores)\n",
    "                wsplit_s.append(number_of_words)\n",
    "                number_of_paragraphs_scores = 1\n",
    "        else:\n",
    "            wsplit_s.append(0)\n",
    "        \n",
    "        last_paragraph_topics_scores = paragraph_topics_scores\n",
    "        last_paragraph_topics_vecs = paragraph_topics_vecs\n",
    "        \n",
    "       \n",
    "    if number_of_paragraphs_vecs > 0:\n",
    "        psplit_v.append(number_of_paragraphs_vecs)\n",
    "        wsplit_v.append(number_of_words)\n",
    "    if number_of_paragraphs_scores > 0:\n",
    "        psplit_s.append(number_of_paragraphs_scores)\n",
    "        wsplit_s.append(number_of_words)\n",
    "    \n",
    "    psplits_v.append(psplit_v)\n",
    "    wsplits_v.append(wsplit_v)\n",
    "    psplits_s.append(psplit_s)\n",
    "    wsplits_s.append(wsplit_s)\n",
    "    progress(i + 1, len(bios_data))\n",
    "bios_data['tst_word_splits_vecs'] = pandas.Series(wsplits_v,index=bios_data.index)\n",
    "bios_data['tst_paragraph_splits_vecs'] = pandas.Series(psplits_v, index=bios_data.index)\n",
    "bios_data['tst_word_splits_ldascores'] = pandas.Series(wsplits_s,index=bios_data.index)\n",
    "bios_data['tst_paragraph_splits_ldascores'] = pandas.Series(psplits_s, index=bios_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>paragraph_splits</th>\n",
       "      <th>person</th>\n",
       "      <th>segments</th>\n",
       "      <th>tokenized_paragraphs</th>\n",
       "      <th>word_splits</th>\n",
       "      <th>tst_word_splits_vecs</th>\n",
       "      <th>tst_paragraph_splits_vecs</th>\n",
       "      <th>tst_word_splits_ldascores</th>\n",
       "      <th>tst_paragraph_splits_ldascores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>Leonard Rossiter</td>\n",
       "      <td>3</td>\n",
       "      <td>[[Leonard, Rossiter, 21, October, 1926, –, 5, ...</td>\n",
       "      <td>[0, 37, 77, 99]</td>\n",
       "      <td>[0, 77, 99, 99]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "      <td>[0, 77, 99, 99]</td>\n",
       "      <td>[1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137</td>\n",
       "      <td>[15]</td>\n",
       "      <td>Jean-Pierre Abbat</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Jean-Pierre, Fernand, Noel, Abbat, June, 17,...</td>\n",
       "      <td>[0, 137]</td>\n",
       "      <td>[0, 79, 108, 113, 118, 135, 136, 137]</td>\n",
       "      <td>[2, 2, 2, 2, 4, 1, 2]</td>\n",
       "      <td>[0, 31, 79, 118, 137]</td>\n",
       "      <td>[1, 1, 6, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>720</td>\n",
       "      <td>[5, 10, 1]</td>\n",
       "      <td>Oscar Niemeyer</td>\n",
       "      <td>3</td>\n",
       "      <td>[[Oscar, Ribeiro, Almeida, Niemeyer, Soares, F...</td>\n",
       "      <td>[0, 356, 668, 720]</td>\n",
       "      <td>[0, 213, 395, 496, 628, 668, 720, 720]</td>\n",
       "      <td>[3, 4, 3, 3, 1, 1, 1]</td>\n",
       "      <td>[0, 385, 612, 628, 668, 720]</td>\n",
       "      <td>[6, 6, 1, 1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>870</td>\n",
       "      <td>[1, 8, 17, 6, 6]</td>\n",
       "      <td>Eudoxus of Cnidus</td>\n",
       "      <td>5</td>\n",
       "      <td>[[Eudoxus, Cnidus, IPAc-en||d||s||s, lang-, ''...</td>\n",
       "      <td>[0, 33, 261, 707, 781, 870]</td>\n",
       "      <td>[0, 314, 521, 554, 870]</td>\n",
       "      <td>[9, 5, 1, 23]</td>\n",
       "      <td>[0, 214, 256, 521, 603, 707, 747, 870]</td>\n",
       "      <td>[5, 2, 7, 5, 6, 3, 10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145</td>\n",
       "      <td>[3]</td>\n",
       "      <td>Jose Saramago</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Jose, Sousa, Saramago, Order, St.GColSE, 16,...</td>\n",
       "      <td>[0, 145]</td>\n",
       "      <td>[0, 145]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[0, 145]</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length  paragraph_splits             person  segments  \\\n",
       "0      99         [1, 1, 1]   Leonard Rossiter         3   \n",
       "1     137              [15]  Jean-Pierre Abbat         1   \n",
       "2     720        [5, 10, 1]     Oscar Niemeyer         3   \n",
       "3     870  [1, 8, 17, 6, 6]  Eudoxus of Cnidus         5   \n",
       "4     145               [3]      Jose Saramago         1   \n",
       "\n",
       "                                tokenized_paragraphs  \\\n",
       "0  [[Leonard, Rossiter, 21, October, 1926, –, 5, ...   \n",
       "1  [[Jean-Pierre, Fernand, Noel, Abbat, June, 17,...   \n",
       "2  [[Oscar, Ribeiro, Almeida, Niemeyer, Soares, F...   \n",
       "3  [[Eudoxus, Cnidus, IPAc-en||d||s||s, lang-, ''...   \n",
       "4  [[Jose, Sousa, Saramago, Order, St.GColSE, 16,...   \n",
       "\n",
       "                   word_splits                    tst_word_splits_vecs  \\\n",
       "0              [0, 37, 77, 99]                         [0, 77, 99, 99]   \n",
       "1                     [0, 137]   [0, 79, 108, 113, 118, 135, 136, 137]   \n",
       "2           [0, 356, 668, 720]  [0, 213, 395, 496, 628, 668, 720, 720]   \n",
       "3  [0, 33, 261, 707, 781, 870]                 [0, 314, 521, 554, 870]   \n",
       "4                     [0, 145]                                [0, 145]   \n",
       "\n",
       "  tst_paragraph_splits_vecs               tst_word_splits_ldascores  \\\n",
       "0                 [1, 1, 1]                         [0, 77, 99, 99]   \n",
       "1     [2, 2, 2, 2, 4, 1, 2]                   [0, 31, 79, 118, 137]   \n",
       "2     [3, 4, 3, 3, 1, 1, 1]            [0, 385, 612, 628, 668, 720]   \n",
       "3             [9, 5, 1, 23]  [0, 214, 256, 521, 603, 707, 747, 870]   \n",
       "4                       [3]                                [0, 145]   \n",
       "\n",
       "  tst_paragraph_splits_ldascores  \n",
       "0                      [1, 1, 1]  \n",
       "1                   [1, 1, 6, 7]  \n",
       "2                [6, 6, 1, 1, 2]  \n",
       "3         [5, 2, 7, 5, 6, 3, 10]  \n",
       "4                            [3]  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our method is pretty good at not over segmenting biographies that have only one segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>paragraph_splits</th>\n",
       "      <th>person</th>\n",
       "      <th>segments</th>\n",
       "      <th>tokenized_paragraphs</th>\n",
       "      <th>word_splits</th>\n",
       "      <th>tst_word_splits_vecs</th>\n",
       "      <th>tst_paragraph_splits_vecs</th>\n",
       "      <th>tst_word_splits_ldascores</th>\n",
       "      <th>tst_paragraph_splits_ldascores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137</td>\n",
       "      <td>[15]</td>\n",
       "      <td>Jean-Pierre Abbat</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Jean-Pierre, Fernand, Noel, Abbat, June, 17,...</td>\n",
       "      <td>[0, 137]</td>\n",
       "      <td>[0, 79, 108, 113, 118, 135, 136, 137]</td>\n",
       "      <td>[2, 2, 2, 2, 4, 1, 2]</td>\n",
       "      <td>[0, 31, 79, 118, 137]</td>\n",
       "      <td>[1, 1, 6, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145</td>\n",
       "      <td>[3]</td>\n",
       "      <td>Jose Saramago</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Jose, Sousa, Saramago, Order, St.GColSE, 16,...</td>\n",
       "      <td>[0, 145]</td>\n",
       "      <td>[0, 145]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[0, 145]</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>238</td>\n",
       "      <td>[4]</td>\n",
       "      <td>Frederick Augustus I of Saxony</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Frederick, Augustus, full, name, ''Frederick...</td>\n",
       "      <td>[0, 238]</td>\n",
       "      <td>[0, 211, 238, 238]</td>\n",
       "      <td>[1, 2, 1]</td>\n",
       "      <td>[0, 238, 238]</td>\n",
       "      <td>[3, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>22</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Fritigern</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Fritigern, Fritigernus, ca, 380, Thervingian...</td>\n",
       "      <td>[0, 22]</td>\n",
       "      <td>[0, 22]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0, 22]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>21</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Victor Lustig</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Victor, Lustig, January, 4, 1890, –, March, ...</td>\n",
       "      <td>[0, 21]</td>\n",
       "      <td>[0, 21]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0, 21]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Randal L. Schwartz</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Randal, L., Schwartz, November, 22, 1961, me...</td>\n",
       "      <td>[0, 12]</td>\n",
       "      <td>[0, 12]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0, 12]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>29</td>\n",
       "      <td>[1]</td>\n",
       "      <td>Carlos Filipe Ximenes Belo</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Carlos, Filipe, Ximenes, Belo, SDB, GCL, 3, ...</td>\n",
       "      <td>[0, 29]</td>\n",
       "      <td>[0, 29]</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[0, 29]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>134</td>\n",
       "      <td>[5]</td>\n",
       "      <td>Danny Kass</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Daniel, Danny, Kass, September, 21, 1982, pr...</td>\n",
       "      <td>[0, 134]</td>\n",
       "      <td>[0, 127, 134]</td>\n",
       "      <td>[3, 2]</td>\n",
       "      <td>[0, 134, 134]</td>\n",
       "      <td>[4, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>89</td>\n",
       "      <td>[3]</td>\n",
       "      <td>Trofim Lysenko</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Trofim, Denisovich, Lysenko, lang-, lang-, s...</td>\n",
       "      <td>[0, 89]</td>\n",
       "      <td>[0, 89]</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[0, 89]</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>144</td>\n",
       "      <td>[4]</td>\n",
       "      <td>Irina Privalova</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Irina, Anatoljewna, Privalova, lang-, nee, S...</td>\n",
       "      <td>[0, 144]</td>\n",
       "      <td>[0, 144]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[0, 144]</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    length paragraph_splits                          person  segments  \\\n",
       "1      137             [15]               Jean-Pierre Abbat         1   \n",
       "4      145              [3]                   Jose Saramago         1   \n",
       "5      238              [4]  Frederick Augustus I of Saxony         1   \n",
       "13      22              [1]                       Fritigern         1   \n",
       "14      21              [1]                   Victor Lustig         1   \n",
       "23      12              [1]              Randal L. Schwartz         1   \n",
       "25      29              [1]      Carlos Filipe Ximenes Belo         1   \n",
       "29     134              [5]                      Danny Kass         1   \n",
       "32      89              [3]                  Trofim Lysenko         1   \n",
       "34     144              [4]                 Irina Privalova         1   \n",
       "\n",
       "                                 tokenized_paragraphs word_splits  \\\n",
       "1   [[Jean-Pierre, Fernand, Noel, Abbat, June, 17,...    [0, 137]   \n",
       "4   [[Jose, Sousa, Saramago, Order, St.GColSE, 16,...    [0, 145]   \n",
       "5   [[Frederick, Augustus, full, name, ''Frederick...    [0, 238]   \n",
       "13  [[Fritigern, Fritigernus, ca, 380, Thervingian...     [0, 22]   \n",
       "14  [[Victor, Lustig, January, 4, 1890, –, March, ...     [0, 21]   \n",
       "23  [[Randal, L., Schwartz, November, 22, 1961, me...     [0, 12]   \n",
       "25  [[Carlos, Filipe, Ximenes, Belo, SDB, GCL, 3, ...     [0, 29]   \n",
       "29  [[Daniel, Danny, Kass, September, 21, 1982, pr...    [0, 134]   \n",
       "32  [[Trofim, Denisovich, Lysenko, lang-, lang-, s...     [0, 89]   \n",
       "34  [[Irina, Anatoljewna, Privalova, lang-, nee, S...    [0, 144]   \n",
       "\n",
       "                     tst_word_splits_vecs tst_paragraph_splits_vecs  \\\n",
       "1   [0, 79, 108, 113, 118, 135, 136, 137]     [2, 2, 2, 2, 4, 1, 2]   \n",
       "4                                [0, 145]                       [3]   \n",
       "5                      [0, 211, 238, 238]                 [1, 2, 1]   \n",
       "13                                [0, 22]                       [1]   \n",
       "14                                [0, 21]                       [1]   \n",
       "23                                [0, 12]                       [1]   \n",
       "25                                [0, 29]                       [1]   \n",
       "29                          [0, 127, 134]                    [3, 2]   \n",
       "32                                [0, 89]                       [3]   \n",
       "34                               [0, 144]                       [4]   \n",
       "\n",
       "   tst_word_splits_ldascores tst_paragraph_splits_ldascores  \n",
       "1      [0, 31, 79, 118, 137]                   [1, 1, 6, 7]  \n",
       "4                   [0, 145]                            [3]  \n",
       "5              [0, 238, 238]                         [3, 1]  \n",
       "13                   [0, 22]                            [1]  \n",
       "14                   [0, 21]                            [1]  \n",
       "23                   [0, 12]                            [1]  \n",
       "25                   [0, 29]                            [1]  \n",
       "29             [0, 134, 134]                         [4, 1]  \n",
       "32                   [0, 89]                            [3]  \n",
       "34                  [0, 144]                            [4]  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios_data.loc[bios_data['segments'].isin([1])][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compare with Alexaner A Alemi and Paul Ginsparg's Method\n",
    "We took the code (https://github.com/alexalemi/segmentation.git) described in this article:\n",
    "http://arxiv.org/pdf/1503.05543v1.pdf  and modified it a little to fit our available embeddings DB and the presentation needs. Running it on the data gives pretty poort results, but can serve as basis for evaluation of our own method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('segmentation/code')\n",
    "from segmentation.code.segmentart import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5999 / 6000 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.segmentation import *\n",
    "def splits_list(bio,ind,acc):\n",
    "    if ind == len(bio)-1:\n",
    "        return acc\n",
    "    elif ind == 0:\n",
    "        acc.append(len(bio[ind][1].split()))\n",
    "    else:\n",
    "        acc.append(acc[ind-1]+len(bio[ind][1].split()))\n",
    "    return splits_list(bio,ind+1,acc)\n",
    "\n",
    "def indexlist2binary(index_list):\n",
    "    ret = \"1\"\n",
    "    for ordinal,split_location in enumerate(index_list):\n",
    "        if ordinal == 0:\n",
    "            continue\n",
    "        ret += \"0\"*(split_location - index_list[ordinal - 1])\n",
    "        ret += \"1\"\n",
    "    return ret\n",
    "\n",
    "alexmi = []\n",
    "ours = []\n",
    "for i in range(len(bios_data)):\n",
    "    onepiece = \" \".join([\" \".join(tp) for tp in bios_data.loc[i,'tokenized_paragraphs']])\n",
    "    gld = bios_data.loc[i,'word_splits']\n",
    "    tst = [0] + segmentize(onepiece,bios_data.loc[i,'segments'],vecs) + [bios_data.loc[i,'length']] if len(gld) > 2 else gld\n",
    "    if len(tst)*len(gld) > 0:\n",
    "        alexmi.append({\n",
    "            'person' : bios_data.loc[i,'person'], \n",
    "            'alexmi pk' : pk(indexlist2binary(gld),indexlist2binary(tst))\n",
    "        })\n",
    "        ours.append({\n",
    "            'person': bios_data.loc[i,'person'],\n",
    "            'our pk vecs' : pk(indexlist2binary(gld),indexlist2binary(bios_data.loc[i,'tst_word_splits_vecs'])),\n",
    "            'our pk lda scores' : pk(indexlist2binary(gld),indexlist2binary(bios_data.loc[i,'tst_word_splits_ldascores']))\n",
    "        })\n",
    "    progress(i, len(bios))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alexmi pk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.321687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.242585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.390072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.513761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.815789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         alexmi pk\n",
       "count  6000.000000\n",
       "mean      0.321687\n",
       "std       0.242585\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.390072\n",
       "75%       0.513761\n",
       "max       0.815789"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexmi = pandas.DataFrame(alexmi)\n",
    "ours = pandas.DataFrame(ours)\n",
    "alexmi.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>our pk lda scores</th>\n",
       "      <th>our pk vecs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6000.000000</td>\n",
       "      <td>6000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.226259</td>\n",
       "      <td>0.241487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.185243</td>\n",
       "      <td>0.193138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.215798</td>\n",
       "      <td>0.235638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.344340</td>\n",
       "      <td>0.366356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.982456</td>\n",
       "      <td>0.966102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       our pk lda scores  our pk vecs\n",
       "count        6000.000000  6000.000000\n",
       "mean            0.226259     0.241487\n",
       "std             0.185243     0.193138\n",
       "min             0.000000     0.000000\n",
       "25%             0.000000     0.000000\n",
       "50%             0.215798     0.235638\n",
       "75%             0.344340     0.366356\n",
       "max             0.982456     0.966102"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ours.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that even though we let the Alexmi code off all one-segment biographies, both our methods mean is still better.\n",
    "But, we can also see that just using the LDA output to gauge the association between a paragraph and every topic, is slightly better than our word embedding method.\n",
    "However, it's probably not good enough for feeding to the segment classifier and getting good results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
