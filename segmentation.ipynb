{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle,sys,os,lda,scipy,pandas\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "bios = []\n",
    "for suf in ['1000']:\n",
    "    bios += pandas.read_pickle(\"train-corpus/corpus\"+suf+\".pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(i, end_val, bar_length=50):\n",
    "    percent = float(i) / end_val\n",
    "    hashes = '#' * int(round(percent * bar_length))\n",
    "    spaces = ' ' * (bar_length - len(hashes))\n",
    "    sys.stdout.write(\"\\r{0} / {1} Percent: [{2}] {3}%\".format(i, end_val, hashes + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Arne Kaijser',\n",
       "  [('Biography',\n",
       "    \"Arne Kaijser (born 1950) is a professor of History of Technology at the Royal Institute of Technology in Stockholm, and the head of the university's department of History of science and technology.\\n\\nKaijser has published two books in Swedish: ''Stadens ljus. Etableringen av de forsta svenska gasverken'' and ''I fadrens spar. Den svenska infrastrukturens historiska utveckling och framtida utmaningar'', and has co-edited several anthologies. Kaijser is a member of the Royal Swedish Academy of Engineering Sciences since 2007 and also a member of the editorial board of two scientific journals: ''Journal of Urban Technology'' and ''Centaurus''. Lately, he has been occupied with the history of Large Technical Systems.\\n\\n\")])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect paragraph data for every biography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001 / 1001 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "bios_df = list()\n",
    "all_tokens = set()\n",
    "i = 0\n",
    "for bio in bios:\n",
    "    data = {\n",
    "        'person' : bio[0][0],\n",
    "        'tokenized_paragraphs' : list(),\n",
    "        'paragraph_splits' : list(),\n",
    "        'word_splits': [0],\n",
    "        'length' : 0,\n",
    "        'segments' : 0\n",
    "    }\n",
    "    number_of_words = 0\n",
    "    for segment in bio[0][1]:\n",
    "        number_of_paragraphs = 0\n",
    "        data['segments'] += 1\n",
    "        for paragraph_text in segment[1].split('\\n'):\n",
    "            tokens = word_tokenize(paragraph_text)\n",
    "            if len(tokens) > 0:\n",
    "                number_of_paragraphs += 1\n",
    "                number_of_words += len(tokens)\n",
    "                all_tokens |= set(tokens)\n",
    "                data['tokenized_paragraphs'].append(tokens)  \n",
    "        data['paragraph_splits'].append(number_of_paragraphs)\n",
    "        data['word_splits'].append(number_of_words)\n",
    "    data['length'] = number_of_words\n",
    "    bios_df.append(data)\n",
    "    i += 1\n",
    "    progress(i, len(bios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>paragraph_splits</th>\n",
       "      <th>person</th>\n",
       "      <th>segments</th>\n",
       "      <th>tokenized_paragraphs</th>\n",
       "      <th>word_splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126</td>\n",
       "      <td>[2]</td>\n",
       "      <td>Arne Kaijser</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Arne, Kaijser, (, born, 1950, ), is, a, prof...</td>\n",
       "      <td>[0, 126]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>669</td>\n",
       "      <td>[4, 13]</td>\n",
       "      <td>Albert Alcibiades, Margrave of Brandenburg-Kul...</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Albert, II, (, Albrecht, ;, 28, March, 1522,...</td>\n",
       "      <td>[0, 71, 669]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length paragraph_splits                                             person  \\\n",
       "0     126              [2]                                       Arne Kaijser   \n",
       "1     669          [4, 13]  Albert Alcibiades, Margrave of Brandenburg-Kul...   \n",
       "\n",
       "   segments                               tokenized_paragraphs   word_splits  \n",
       "0         1  [[Arne, Kaijser, (, born, 1950, ), is, a, prof...      [0, 126]  \n",
       "1         2  [[Albert, II, (, Albrecht, ;, 28, March, 1522,...  [0, 71, 669]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios_data = pandas.DataFrame(bios_df)\n",
    "bios_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14031 / 14031 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "all_tokens_list = list(all_tokens)\n",
    "number_of_tokens = len(all_tokens_list)\n",
    "all_paragraphs = bios_data['tokenized_paragraphs'].sum()\n",
    "paragraphs_bow = np.zeros([len(all_paragraphs),number_of_tokens], dtype = np.int)\n",
    "tokens_indices_dict = dict()\n",
    "for i in range(number_of_tokens):\n",
    "    tokens_indices_dict[all_tokens_list[i]] = i\n",
    "    \n",
    "for i in range(len(all_paragraphs)):\n",
    "    for w in all_paragraphs[i]:\n",
    "        paragraphs_bow[i][tokens_indices_dict[w]] += 1\n",
    "        \n",
    "    progress(i + 1, len(all_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign LDA topics to paragraphs with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14031, 59063)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(all_tokens)\n",
    "paragraphs_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)\n",
    "#model.fit(paragraphs_bow)  # model.fit_transform(X) is also available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with open('toy_topics.pkl','wb') as toy:\n",
    "#    pickle.dump(model,toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pandas.read_pickle('toy_topics.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecs = pickle.load(open('/home/ilay/vecs.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paragraph_to_vector(paragraph_tokens):\n",
    "    l = len(vecs['queen']) # len of the vector is 300\n",
    "    paragraph_accumulative = np.zeros(l)\n",
    "    topic_ratings = []\n",
    "    # just sum the paragraph words' vectors to get a semantic average of it\n",
    "    for ind,word in enumerate(paragraph_tokens):\n",
    "        if word in vecs:\n",
    "            paragraph_accumulative += vecs[word]\n",
    "    return paragraph_accumulative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate topic lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: walks cartographical ever-flowing Shu cross-dressing sociologist Cuckoo Geoffroy Mysorean ''Quarterly historic unresponsive subtleties 19521962 combinatorial\n",
      "Topic 1: ever-flowing Geoffroy walks Shu sociologist cross-dressing fourth Marguerite 1.04 obeyed ''Quarterly proposal Marinetti Cuckoo Granada\n",
      "Topic 2: walks Shu Armored sociologist cross-dressing Smaragdus Cuckoo ever-flowing Jullian Geoffroy subtleties Marguerite fourth counterbalanced 19521962\n",
      "Topic 3: ever-flowing Geoffroy walks Shu sociologist cross-dressing Marguerite Cuckoo unresponsive Mysorean fourth proposal subtleties Cooper compulsory\n",
      "Topic 4: walks Shu ever-flowing cartographical Peckham historic fourth Geoffroy Cuckoo sale cross-dressing sociologist tenants Donatism aegis\n",
      "Topic 5: ever-flowing fourth walks Shu Geoffroy cross-dressing Cuckoo obeyed Marguerite Peckham sociologist Marinetti Shaa expectations 19521962\n",
      "Topic 6: cartographical walks 3.9 Austrians fatigues orations Shaa Improvised Jailhouse currykrysset biochemist misleadingly Doctorow ''Batman Hugh\n",
      "Topic 7: ever-flowing walks Geoffroy cross-dressing Shu sociologist Cuckoo fourth ''Board Shaa encounter ''Quarterly subtleties t=|s=|p=first=t Marguerite\n",
      "Topic 8: ever-flowing Geoffroy walks Shu cross-dressing cartographical sociologist Cuckoo sale fourth unresponsive Austrians 3.9 obeyed Marguerite\n",
      "Topic 9: walks Geoffroy ever-flowing cross-dressing Shu obeyed sociologist Cuckoo unresponsive cartographical subtleties historic 19521962 Mysorean fourth\n",
      "Topic 10: Geoffroy walks ever-flowing Shu cross-dressing sociologist Marguerite obeyed Apologetic subtleties Video unresponsive Oshii fourth Cooper\n",
      "Topic 11: cartographical 3.9 Austrians fatigues walks ''Board Caixa orations historic kneel exakten subtleties Terrasse 34-year-old 19521962\n",
      "Topic 12: walks Shu sociologist Marinetti obeyed cross-dressing fourth ever-flowing Cuckoo Marguerite counterbalanced Geoffroy proposal orchestras 19521962\n",
      "Topic 13: cartographical Austrians 3.9 orations fatigues Geoffroy studying Struik Shu Mysorean walks Caixa ''Board easing Shaa\n",
      "Topic 14: walks Shu cartographical orations fatigues kneel 3.9 Austrians Geoffroy Archdiocese cross-dressing IPAc-en||||n ''Board exakten sociologist\n",
      "Topic 15: Geoffroy ever-flowing walks Shu cross-dressing sociologist Cuckoo inhabitants proposal Mysorean counterbalanced fourth ''Quarterly sale Marguerite\n",
      "Topic 16: ever-flowing walks sociologist Geoffroy Shu counterbalanced subtleties ''Quarterly sale ''Board minting Cuckoo Marguerite abundance Shaa\n",
      "Topic 17: 3.9 Austrians walks Shu Marguerite cross-dressing friendly kneel sale Cuckoo *William proposal Tripolitania Leumites ''Winnower\n",
      "Topic 18: ever-flowing walks Shu sociologist cross-dressing Geoffroy Cuckoo Marguerite Mysorean counterbalanced fourth subtleties ''Quarterly orchestras 19521962\n",
      "Topic 19: ever-flowing walks Geoffroy Shu cross-dressing sociologist Marguerite obeyed fourth Cuckoo proposal Wiedensahl 3.9 unresponsive Cooper\n"
     ]
    }
   ],
   "source": [
    "topic_word = model.topic_word_ \n",
    "n_top_words = 20\n",
    "\n",
    "topics_words = list()\n",
    "\n",
    "word_freqs = dict()\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    # topic_words: words sorted by relevance to topic in descending order\n",
    "    topic_words = list(np.array(vocab)[np.argsort(topic_dist)[::-1]])#[:10]#[:-(n_top_words+1):-1]\n",
    "    #print(topic_words)\n",
    "    topics_words.append(topic_words)\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words[:15])))\n",
    "    for word in topic_words[:20]:\n",
    "        if word not in word_freqs:\n",
    "            word_freqs[word] = 1\n",
    "        else:\n",
    "            word_freqs[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_words_indices_dicts = list()\n",
    "for topic_words in topics_words:\n",
    "    topic_words_indices_dict = dict()\n",
    "    for i in range(len(topic_words)):\n",
    "        topic_words_indices_dict[topic_words[i]] = i\n",
    "    topic_words_indices_dicts.append(topic_words_indices_dict)\n",
    "\n",
    "def calculate_coherence_score(sentences_topics_frequencies, number_of_topics, position, window_size):\n",
    "    pre_topics_frequencies = np.zeros(number_of_topics)\n",
    "    for i in range(window_size):\n",
    "        pre_topics_frequencies += sentences_topics_frequencies[position - i]\n",
    "        \n",
    "    post_topics_frequencies = np.zeros(number_of_topics)\n",
    "    for i in range(window_size):\n",
    "        post_topics_frequencies += sentences_topics_frequencies[position + 1 + i]\n",
    "    return scipy.spatial.distance.cosine(pre_topics_frequencies, post_topics_frequencies) \n",
    "\n",
    "def calculate_depth_scores(coherence_scores):\n",
    "    depth_scores = list()\n",
    "    for i in range(len(coherence_scores)):\n",
    "        hl = coherence_scores[i]\n",
    "        hr = coherence_scores[i]\n",
    "        for j in range(i):\n",
    "            if coherence_scores[j] > coherence_scores[i]:\n",
    "                hl = coherence_scores[j]\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        for j in range(i + 1, len(coherence_scores)):\n",
    "            if coherence_scores[j] > coherence_scores[i]:\n",
    "                hr = coherence_scores[j]\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        depth_score = 0.5 * (hl + hr - 2 * coherence_scores[i])\n",
    "        depth_scores.append(depth_score)\n",
    "        \n",
    "    return depth_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_word_topic_id(word, topics_words):\n",
    "    if word in word_freqs and word_freqs[word] > 1:\n",
    "        return -1\n",
    "    \n",
    "    topic_id = -1\n",
    "    min_topic_index = number_of_tokens + 1\n",
    "    for current_topic_id in range(number_of_topics):\n",
    "        index = topic_words_indices_dicts[current_topic_id][word]\n",
    "        if index < min_topic_index:\n",
    "            topic_id = current_topic_id\n",
    "            min_topic_index = index\n",
    "            \n",
    "    return topic_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_vectors = list()\n",
    "for topic_words in topics_words:\n",
    "    words_taken = 0\n",
    "    i = 0\n",
    "    vector = np.zeros(300)\n",
    "    # take first 200 out of the 59,000 words\n",
    "    while(words_taken < 200):\n",
    "        word = topic_words[i]\n",
    "        if (word not in word_freqs or word_freqs[word] < 5) and word in vecs:\n",
    "            vector += vecs[word]\n",
    "            words_taken += 1\n",
    "        i += 1\n",
    "    topic_vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a list of topics for each paragraph by distance of topic vectors from the paragraph vector\n",
    "def paragraph_topics_rating(paragraph,topic_vectors):\n",
    "    cosine = scipy.spatial.distance.cosine\n",
    "    return np.argsort([cosine(paragraph_to_vector(paragraph),topic_vector) for i, topic_vector in enumerate(topic_vectors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6,  7, 14, 15, 12,  2, 11, 16,  3,  1,  0, 13,  9,  4,  5, 19, 17,\n",
       "       10, 18,  8])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_topics_rating(bios_data.loc[1,'tokenized_paragraphs'][0],topic_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means topic 6 is most strongly linked to this pargraph by the aggregate vectors assessment, then topic 7, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the text by according to the topic ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "psplits = list()\n",
    "wsplits = list()\n",
    "for i in range(len(bios_data)):\n",
    "    last_paragraph_topics = np.array([])\n",
    "    number_of_paragraphs = 1\n",
    "    number_of_words = 0\n",
    "    psplit = list()\n",
    "    wsplit = list()\n",
    "    for tp in bios_data.loc[i,'tokenized_paragraphs']:\n",
    "        number_of_words += len(tp)\n",
    "        paragraph_topics = paragraph_topics_rating(tp,topic_vectors)[:3]\n",
    "        if len(last_paragraph_topics) > 0:\n",
    "            if len(np.intersect1d(paragraph_topics, last_paragraph_topics)) > 0:\n",
    "                number_of_paragraphs += 1\n",
    "            else:\n",
    "                psplit.append(number_of_paragraphs)\n",
    "                wsplit.append(number_of_words)\n",
    "                number_of_paragraphs = 1\n",
    "        else:\n",
    "            wsplit.append(0)\n",
    "        last_paragraph_topics = paragraph_topics\n",
    "        \n",
    "       \n",
    "    if number_of_paragraphs > 0:\n",
    "        psplit.append(number_of_paragraphs)\n",
    "        wsplit.append(number_of_words)\n",
    "    psplits.append(psplit)\n",
    "    wsplits.append(wsplit)\n",
    "bios_data['tst_word_splits'] = pandas.Series(wsplits,index=bios_data.index)\n",
    "bios_data['tst_paragraph_splits'] = pandas.Series(psplits, index=bios_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy as KL\n",
    "def paragraph_pairs(bio):\n",
    "    stats = list()\n",
    "    \n",
    "    number_of_paragraphs = 1\n",
    "    for i in range(len(bio) - 1):\n",
    "        paragraph1 = bio[i]\n",
    "        paragraph2 = bio[i + 1]\n",
    "        shulman_distance = Shulman_Distance(paragraph_topics_rating(paragraph1,topic_vectors),\n",
    "                               paragraph_topics_rating(paragraph2,topic_vectors))\n",
    "        print((i + 1), \" \".join(bio[i][:10]))\n",
    "        print()\n",
    "        print(shulman_distance)\n",
    "        print()\n",
    "    \n",
    "        last_shulman_distance = shulman_distance\n",
    "        \n",
    "import scipy    \n",
    "def flip_array(a):\n",
    "    flipped = [0] * len(a)\n",
    "    #print(len(flipped))\n",
    "    for i in range(len(a)):\n",
    "        flipped[a[i]] = i\n",
    "    return np.array(flipped)\n",
    "        \n",
    "def Shulman_Distance(v1,v2):\n",
    "    v1flip = flip_array(v1)\n",
    "    v2flip = flip_array(v2)\n",
    "    diff = v1flip - v2flip\n",
    "    return scipy.linalg.norm(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>paragraph_splits</th>\n",
       "      <th>person</th>\n",
       "      <th>segments</th>\n",
       "      <th>tokenized_paragraphs</th>\n",
       "      <th>word_splits</th>\n",
       "      <th>tst_word_splits</th>\n",
       "      <th>tst_paragraph_splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126</td>\n",
       "      <td>[2]</td>\n",
       "      <td>Arne Kaijser</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Arne, Kaijser, (, born, 1950, ), is, a, prof...</td>\n",
       "      <td>[0, 126]</td>\n",
       "      <td>[0, 126]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>669</td>\n",
       "      <td>[4, 13]</td>\n",
       "      <td>Albert Alcibiades, Margrave of Brandenburg-Kul...</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Albert, II, (, Albrecht, ;, 28, March, 1522,...</td>\n",
       "      <td>[0, 71, 669]</td>\n",
       "      <td>[0, 669]</td>\n",
       "      <td>[17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>239</td>\n",
       "      <td>[4]</td>\n",
       "      <td>Aimoin</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Aimoin, (, circa, 960, c., 1010, ), ,, Frenc...</td>\n",
       "      <td>[0, 239]</td>\n",
       "      <td>[0, 239]</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>314</td>\n",
       "      <td>[4]</td>\n",
       "      <td>Alcamenes</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Alcamenes, (, lang-grc|, ), was, an, ancient...</td>\n",
       "      <td>[0, 314]</td>\n",
       "      <td>[0, 314]</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>270</td>\n",
       "      <td>[1, 4]</td>\n",
       "      <td>Alexander Balas</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Alexander, Balas, (, o, B, ), ,, ruler, of, ...</td>\n",
       "      <td>[0, 17, 270]</td>\n",
       "      <td>[0, 93, 270]</td>\n",
       "      <td>[1, 4]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length paragraph_splits                                             person  \\\n",
       "0     126              [2]                                       Arne Kaijser   \n",
       "1     669          [4, 13]  Albert Alcibiades, Margrave of Brandenburg-Kul...   \n",
       "2     239              [4]                                             Aimoin   \n",
       "3     314              [4]                                          Alcamenes   \n",
       "4     270           [1, 4]                                    Alexander Balas   \n",
       "\n",
       "   segments                               tokenized_paragraphs   word_splits  \\\n",
       "0         1  [[Arne, Kaijser, (, born, 1950, ), is, a, prof...      [0, 126]   \n",
       "1         2  [[Albert, II, (, Albrecht, ;, 28, March, 1522,...  [0, 71, 669]   \n",
       "2         1  [[Aimoin, (, circa, 960, c., 1010, ), ,, Frenc...      [0, 239]   \n",
       "3         1  [[Alcamenes, (, lang-grc|, ), was, an, ancient...      [0, 314]   \n",
       "4         2  [[Alexander, Balas, (, o, B, ), ,, ruler, of, ...  [0, 17, 270]   \n",
       "\n",
       "  tst_word_splits tst_paragraph_splits  \n",
       "0        [0, 126]                  [2]  \n",
       "1        [0, 669]                 [17]  \n",
       "2        [0, 239]                  [4]  \n",
       "3        [0, 314]                  [4]  \n",
       "4    [0, 93, 270]               [1, 4]  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compare with Alexaner A Alemi and Paul Ginsparg's Method\n",
    "We took the code (https://github.com/alexalemi/segmentation.git) described in this article:\n",
    "http://arxiv.org/pdf/1503.05543v1.pdf  and modified it a little to fit our available embeddings DB and the presentation needs. Running it on the data gives pretty poort results, but can serve as basis for evaluation of our own method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('segmentation/code')\n",
    "from segmentation.code.segmentart import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 / 1001 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.segmentation import *\n",
    "def splits_list(bio,ind,acc):\n",
    "    if ind == len(bio)-1:\n",
    "        return acc\n",
    "    elif ind == 0:\n",
    "        acc.append(len(bio[ind][1].split()))\n",
    "    else:\n",
    "        acc.append(acc[ind-1]+len(bio[ind][1].split()))\n",
    "    return splits_list(bio,ind+1,acc)\n",
    "\n",
    "def indexlist2binary(index_list):\n",
    "    ret = \"1\"\n",
    "    for ordinal,split_location in enumerate(index_list):\n",
    "        if ordinal == 0:\n",
    "            continue\n",
    "        ret += \"0\"*(split_location - index_list[ordinal - 1])\n",
    "        ret += \"1\"\n",
    "    return ret\n",
    "\n",
    "alexmi = []\n",
    "ours = []\n",
    "for i in range(len(bios_data)):\n",
    "    onepiece = \" \".join([\" \".join(tp) for tp in bios_data.loc[i,'tokenized_paragraphs']])\n",
    "    gld = bios_data.loc[i,'word_splits']\n",
    "    tst = [0] + segmentize(onepiece,bios_data.loc[i,'segments'],vecs) + [bios_data.loc[i,'length']] if len(gld) > 2 else gld\n",
    "    if len(tst)*len(gld) > 0:\n",
    "        alexmi.append({\n",
    "            'person' : bios_data.loc[i,'person'], \n",
    "            'alexmi pk' : pk(indexlist2binary(gld),indexlist2binary(tst))\n",
    "        })\n",
    "        ours.append({\n",
    "                'person': bios_data.loc[i,'person'],\n",
    "                'our pk' : pk(indexlist2binary(gld),indexlist2binary(bios_data.loc[i,'tst_word_splits']))\n",
    "            })\n",
    "    progress(i, len(bios))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alexmi pk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1001.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.345076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.235191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.415364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.523810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.780822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         alexmi pk\n",
       "count  1001.000000\n",
       "mean      0.345076\n",
       "std       0.235191\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.415364\n",
       "75%       0.523810\n",
       "max       0.780822"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexmi = pandas.DataFrame(alexmi)\n",
    "ours = pandas.DataFrame(ours)\n",
    "alexmi.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>our pk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1001.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.218856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.174678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.019417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.207207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.327860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.903361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            our pk\n",
       "count  1001.000000\n",
       "mean      0.218856\n",
       "std       0.174678\n",
       "min       0.000000\n",
       "25%       0.019417\n",
       "50%       0.207207\n",
       "75%       0.327860\n",
       "max       0.903361"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ours.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that even though we let the Alexmi code off all one-segment biographies, our method's mean is still better.\n",
    "Sadly, it's probably not good enough for feeding to the segment classifier for getting any reasonable results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
