{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle,sys,os,lda,scipy,pandas\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "bios = []\n",
    "for suf in ['18000']:\n",
    "    bios += pandas.read_pickle(\"train-corpus/corpus\"+suf+\".pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(i, end_val, bar_length=50):\n",
    "    percent = float(i) / end_val\n",
    "    hashes = '#' * int(round(percent * bar_length))\n",
    "    spaces = ' ' * (bar_length - len(hashes))\n",
    "    sys.stdout.write(\"\\r{0} / {1} Percent: [{2}] {3}%\".format(i, end_val, hashes + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John Renshaw Starr',\n",
       "  [('Summary',\n",
       "    \"John Renshaw Starr (died 1996), was one of two sons of Alfred Demarest Starr (an American) and Ethel Renshaw (English). He was a grandson of William Robert Renshaw. He was an artist and a soldier during the Second World War. His story is told in a book, ''The Starr Affair'', by Jean Overton Fuller.\\n\\n\"),\n",
       "   ('Release',\n",
       "    'By exploiting his ability to pass himself off as a Frenchman, he joined a group of French and Belgian prisoners who were released into the custody of the Red Cross and taken to Switzerland as the war in Europe drew to a close.\\n\\nStories from other SOE agents who shared his captivity at the Avenue Foch resulted in doubts being raised about his loyalty, and his case became the subject of an MI5 investigation, which concluded that although his behaviour was certainly suspicious, there were no grounds for criminal prosecution. \\n\\n')])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find noise words (in order to filter them later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 / 1000 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "tokens_freqs = dict()\n",
    "\n",
    "i = 0\n",
    "for bio in bios:\n",
    "    for segment in bio[0][1]:\n",
    "        \n",
    "        for paragraph_text in segment[1].split('\\n'):\n",
    "            tokens = word_tokenize(paragraph_text)\n",
    "            for token in tokens:\n",
    "                if token in tokens_freqs:\n",
    "                    tokens_freqs[token] += 1\n",
    "                else:\n",
    "                    tokens_freqs[token] = 1\n",
    "    i += 1\n",
    "    progress(i, len(bios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "# Sorder tokens by frequency in reverse order (most frequent first):\n",
    "most_frequent_tokens = sorted(tokens_freqs.items(), key=operator.itemgetter(1))\n",
    "most_frequent_tokens.reverse()\n",
    "\n",
    "noise_words = set()\n",
    "for token_and_freq in most_frequent_tokens:\n",
    "    # We take the threshold to be 1/100 the frequency of the most frequnent token:\n",
    "    if token_and_freq[1] > most_frequent_tokens[0][1] / 100:\n",
    "        noise_words.add(token_and_freq[0])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect paragraph data for every biography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 / 1000 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "bios_df = list()\n",
    "all_tokens = set()\n",
    "i = 0\n",
    "for bio in bios:\n",
    "    data = {\n",
    "        'person' : bio[0][0],\n",
    "        'tokenized_paragraphs' : list(),\n",
    "        'paragraph_splits' : list(),\n",
    "        'word_splits': [0],\n",
    "        'length' : 0,\n",
    "        'segments' : 0\n",
    "    }\n",
    "    number_of_words = 0\n",
    "    for segment in bio[0][1]:\n",
    "        number_of_paragraphs = 0\n",
    "        data['segments'] += 1\n",
    "        for paragraph_text in segment[1].split('\\n'):\n",
    "            original_tokens = word_tokenize(paragraph_text)\n",
    "            tokens = list()\n",
    "            for token in original_tokens:\n",
    "                if token not in noise_words:\n",
    "                    tokens.append(token)\n",
    "            \n",
    "            if len(tokens) > 0:\n",
    "                number_of_paragraphs += 1\n",
    "                number_of_words += len(tokens)\n",
    "                all_tokens |= set(tokens)\n",
    "                data['tokenized_paragraphs'].append(tokens)  \n",
    "        data['paragraph_splits'].append(number_of_paragraphs)\n",
    "        data['word_splits'].append(number_of_words)\n",
    "    data['length'] = number_of_words\n",
    "    bios_df.append(data)\n",
    "    i += 1\n",
    "    progress(i, len(bios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>paragraph_splits</th>\n",
       "      <th>person</th>\n",
       "      <th>segments</th>\n",
       "      <th>tokenized_paragraphs</th>\n",
       "      <th>word_splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>176</td>\n",
       "      <td>[1, 4]</td>\n",
       "      <td>Samuel Cornish</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Samuel, Eli, Cornish, 1795, 6, 1858, Presbyt...</td>\n",
       "      <td>[0, 34, 176]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>73</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>John Renshaw Starr</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Renshaw, Starr, 1996, sons, Alfred, Demarest...</td>\n",
       "      <td>[0, 25, 73]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length paragraph_splits              person  segments  \\\n",
       "0     176           [1, 4]      Samuel Cornish         2   \n",
       "1      73           [1, 2]  John Renshaw Starr         2   \n",
       "\n",
       "                                tokenized_paragraphs   word_splits  \n",
       "0  [[Samuel, Eli, Cornish, 1795, 6, 1858, Presbyt...  [0, 34, 176]  \n",
       "1  [[Renshaw, Starr, 1996, sons, Alfred, Demarest...   [0, 25, 73]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios_data = pandas.DataFrame(bios_df)\n",
    "bios_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10005 / 10272 Percent: [################################################# ] 97%"
     ]
    }
   ],
   "source": [
    "all_tokens_list = list(all_tokens)\n",
    "number_of_tokens = len(all_tokens_list)\n",
    "all_paragraphs = bios_data['tokenized_paragraphs'].sum()\n",
    "paragraphs_bow = np.zeros([len(all_paragraphs),number_of_tokens], dtype = np.int)\n",
    "tokens_indices_dict = dict()\n",
    "for i in range(number_of_tokens):\n",
    "    tokens_indices_dict[all_tokens_list[i]] = i\n",
    "    \n",
    "for i in range(len(all_paragraphs)):\n",
    "    for w in all_paragraphs[i]:\n",
    "        paragraphs_bow[i][tokens_indices_dict[w]] += 1\n",
    "        \n",
    "    progress(i + 1, len(all_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign LDA topics to paragraphs with word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deprecated (we use Gensim LDA instead):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10272, 45633)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(all_tokens)\n",
    "paragraphs_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA at 0x10ce15b00>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)\n",
    "model.fit(paragraphs_bow)  # model.fit_transform(X) is also available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with open('toy_topics.pkl','wb') as toy:\n",
    "#    pickle.dump(model,toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = pandas.read_pickle('toy_topics.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate topic lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: role television appeared films Award Best played starred actor comedy show character won Theatre roles\n",
      "Topic 1: band album composer songs Orchestra song Music recorded released orchestra performed early conductor guitar piano\n",
      "Topic 2: English wrote William French book England poetry Irish Society Thomas poet Great Charles under written\n",
      "Topic 3: League Hall former BBC show Fame 2011 2009 season team 2008 Radio Baseball 2003 Major\n",
      "Topic 4: 2002 Hackenschmidt won 1999 Green Gotch career $ home 2004 runs finished received major professional\n",
      "Topic 5: October – February July April August British politician 13 11 William 12 17 27 Quebec\n",
      "Topic 6: Duke King Roman Bolesaw Poland III father brother name Wadysaw Emperor king Rome Saint Kossuth\n",
      "Topic 7: ISBN ? You ! Love Press book ''A 1995 Other My 2000 Times Is Life\n",
      "Topic 8: company used Ford Company painting designed industry use produced new engineer design machine developed created\n",
      "Topic 9: out did said up so than them life no being some could new because there\n",
      "Topic 10: Hong Wei Chinese Kong Emperor name Sun murder Yan Yi under Kangxi Empress Wong apartment\n",
      "Topic 11: such works history wrote theory book important century considered often well social between This early\n",
      "Topic 12: President Senate Party elected House election Minister government former Republican political Rudd state % National\n",
      "Topic 13: Clan di CLN File e J La Lope Kropp Spanish Jolly Comanche / da Anderson\n",
      "Topic 14: German Russian Germany Nelson title Soviet C File Berlin Dutch der Category Kennan agents Image\n",
      "Topic 15: Army military General war command men officer Union U.S. Mosby forces Battle general army killed\n",
      "Topic 16: daughter Lady Duke Henry % children Charles Count Lord wife Prince Maria Princess 4 William\n",
      "Topic 17: named father City family age school moved Church buried County California wife home now children\n",
      "Topic 18: College received Royal Academy Award Order School Cambridge National professor degree Society Institute awarded Science\n",
      "Topic 19: Open round 46 36 64 26 lost 63 losing 67 57 76 No 16 Safin\n"
     ]
    }
   ],
   "source": [
    "topic_word = model.topic_word_ \n",
    "n_top_words = 20\n",
    "\n",
    "topics_words = list()\n",
    "\n",
    "# word_freqs = dict()\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    # topic_words: words sorted by relevance to topic in descending order\n",
    "    topic_words = list(np.array(vocab)[np.argsort(topic_dist)[::-1]])#[:10]#[:-(n_top_words+1):-1]\n",
    "    #print(topic_words)\n",
    "    topics_words.append(topic_words)\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words[:15])))\n",
    "#     for word in topic_words[:20]:\n",
    "#         if word not in word_freqs:\n",
    "#             word_freqs[word] = 1\n",
    "#         else:\n",
    "#             word_freqs[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using gensim lda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(all_paragraphs)\n",
    "corpus = [dictionary.doc2bow(text) for text in all_paragraphs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "lda = LdaModel(corpus, id2word=dictionary, num_topics=20, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: composer wrote October actor early English best television lawyer performed works 2012 Opera April songs\n",
      "Topic 2: Lloyd book life science computer history Anderson fiction based books novel collection several written England\n",
      "Topic 3: round 46 36 64 Cup Japanese Safin match both Japan team won ! Hackenschmidt Ferrero\n",
      "Topic 4: Taylor Atari 62 worked day frequently age services name Robinett effort make Orville 1990 game\n",
      "Topic 5: Records CD Orchestra Award Music album Hell 2008 DJ Gigolo Best 2005 Deejay Other engine\n",
      "Topic 6: hit among 2014 art Los school Angeles Daniel California high 2011 McPhee Abbott began friends\n",
      "Topic 7: 63 Prince ''International France Masters Spain River Philip LP Luis back war troops brother upon\n",
      "Topic 8: Arminius Miller July Prime February jazz 2015 Harry ** 1983 acting Day Funes Live opera\n",
      "Topic 9: Music Hall playing piano Life *The performances Is marriage Fame Love Lovely Paul 2003 Blue\n",
      "Topic 10: Quebec Mercier Lady works Jersey composers yet Mary buried title Dreyblatt 1953 What use project\n",
      "Topic 11: government County man some own bill said church allowed $ out being Rudd Church father\n",
      "Topic 12: President National College Party Academy elected appointed School Art Royal received U.S. Institute House Arts\n",
      "Topic 13: did up than because This new such so being several any no group together psychology\n",
      "Topic 14: Open 26 Batlle B lost ''Disko losing 57 16 leader Army Mosby George Australian Robert\n",
      "Topic 15: Press ISBN 67 2009 Berlin ''A Hemmings Center Durer Berkeley historian Bar Sherman 2004 Table\n",
      "Topic 16: daughter Charles theory 8 Duke William 2 % children wife Wrights 25 Henry built Lord\n",
      "Topic 17: Panofsky Princeton Wright met Davis studies Jewish took NPR up La ''Early Nicosia Netherlandish 1934\n",
      "Topic 18: art Frazetta artist artists electronic such From Irving plane aircraft next 1956 influenced face notable\n",
      "Topic 19: role played League appeared band television 4 2006 show 2009 career 2010 Theatre starred Major\n",
      "Topic 20: concert David public computing helped ideas Roman Raymond media mathematical well Carolina money / there\n"
     ]
    }
   ],
   "source": [
    "topics_words = list()\n",
    "\n",
    "i = 0\n",
    "for topic in lda.show_topics(num_topics=20, formatted=False, num_words=len(all_tokens)):\n",
    "    i += 1\n",
    "    topic_words = list()\n",
    "    for score, word in topic:\n",
    "        topic_words.append(word)\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words[:15])))\n",
    "    topics_words.append(topic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecs = pickle.load(open('/home/ilay/vecs.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paragraph_to_vector(paragraph_tokens):\n",
    "    l = len(vecs['queen']) # len of the vector is 300\n",
    "    paragraph_accumulative = np.zeros(l)\n",
    "    topic_ratings = []\n",
    "    # just sum the paragraph words' vectors to get a semantic average of it\n",
    "    for ind,word in enumerate(paragraph_tokens):\n",
    "        if word in vecs:\n",
    "            paragraph_accumulative += vecs[word]\n",
    "    return paragraph_accumulative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each topic, make a representing vector by summing it's first 200 word-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_vectors = list()\n",
    "for topic_words in topics_words:\n",
    "    words_taken = 0\n",
    "    i = 0\n",
    "    vector = np.zeros(300)\n",
    "    while(words_taken < 200):\n",
    "        word = topic_words[i]\n",
    "#         if (word not in word_freqs or word_freqs[word] < 5) and word in vecs:\n",
    "        if word in vecs:\n",
    "            vector += vecs[word]\n",
    "            words_taken += 1\n",
    "        i += 1\n",
    "    topic_vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a list of topics for each paragraph by distance of topic vectors from the paragraph vector\n",
    "def paragraph_topics_rating(paragraph,topic_vectors):\n",
    "    cosine = scipy.spatial.distance.cosine\n",
    "    return np.argsort([cosine(paragraph_to_vector(paragraph),topic_vector) for i, topic_vector in enumerate(topic_vectors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  7, 12, 16,  9,  3,  6,  4, 14, 13,  0,  1, 10, 19,  2, 15,  5,\n",
       "        8, 18, 17])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_topics_rating(bios_data.loc[1,'tokenized_paragraphs'][0],topic_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means topic 11 is most strongly linked to this pargraph, then topic 7, then 12 etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the text using according to the topic ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 / 1000 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "psplits = list()\n",
    "wsplits = list()\n",
    "for i in range(len(bios_data)):\n",
    "    last_paragraph_topics = np.array([])\n",
    "    number_of_paragraphs = 1\n",
    "    number_of_words = 0\n",
    "    psplit = list()\n",
    "    wsplit = list()\n",
    "    for tp in bios_data.loc[i,'tokenized_paragraphs']:\n",
    "        number_of_words += len(tp)\n",
    "        paragraph_topics = paragraph_topics_rating(tp,topic_vectors)[:3]\n",
    "        if len(last_paragraph_topics) > 0:\n",
    "            if len(np.intersect1d(paragraph_topics, last_paragraph_topics)) > 0:\n",
    "                number_of_paragraphs += 1\n",
    "            else:\n",
    "                psplit.append(number_of_paragraphs)\n",
    "                wsplit.append(number_of_words)\n",
    "                number_of_paragraphs = 1\n",
    "        else:\n",
    "            wsplit.append(0)\n",
    "        last_paragraph_topics = paragraph_topics\n",
    "        \n",
    "       \n",
    "    if number_of_paragraphs > 0:\n",
    "        psplit.append(number_of_paragraphs)\n",
    "        wsplit.append(number_of_words)\n",
    "    psplits.append(psplit)\n",
    "    wsplits.append(wsplit)\n",
    "    progress(i + 1, len(bios_data))\n",
    "bios_data['tst_word_splits'] = pandas.Series(wsplits,index=bios_data.index)\n",
    "bios_data['tst_paragraph_splits'] = pandas.Series(psplits, index=bios_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>paragraph_splits</th>\n",
       "      <th>person</th>\n",
       "      <th>segments</th>\n",
       "      <th>tokenized_paragraphs</th>\n",
       "      <th>word_splits</th>\n",
       "      <th>tst_word_splits</th>\n",
       "      <th>tst_paragraph_splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>407</td>\n",
       "      <td>[1, 4]</td>\n",
       "      <td>Samuel Cornish</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Samuel, Eli, Cornish, (, 1795, 6, November, ...</td>\n",
       "      <td>[0, 89, 407]</td>\n",
       "      <td>[0, 407]</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>164</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>John Renshaw Starr</td>\n",
       "      <td>2</td>\n",
       "      <td>[[John, Renshaw, Starr, (, died, 1996, ), ,, w...</td>\n",
       "      <td>[0, 68, 164]</td>\n",
       "      <td>[0, 113, 164]</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>George Reginald Starr</td>\n",
       "      <td>2</td>\n",
       "      <td>[[George, Reginald, Starr, DSO, MC, (, 6, Apri...</td>\n",
       "      <td>[0, 34, 85]</td>\n",
       "      <td>[0, 73, 85]</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>306</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>Claire Windsor, Countess of Ulster</td>\n",
       "      <td>2</td>\n",
       "      <td>[[''Gloucester, family, banner, '', name, =mar...</td>\n",
       "      <td>[0, 264, 306]</td>\n",
       "      <td>[0, 145, 204, 306]</td>\n",
       "      <td>[2, 1, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>Tom Campbell (California politician)</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Thomas, John, ``, Tom, '', Campbell, (, born...</td>\n",
       "      <td>[0, 161, 171]</td>\n",
       "      <td>[0, 171]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length paragraph_splits                                person  segments  \\\n",
       "0     407           [1, 4]                        Samuel Cornish         2   \n",
       "1     164           [1, 2]                    John Renshaw Starr         2   \n",
       "2      85           [1, 2]                 George Reginald Starr         2   \n",
       "3     306           [5, 1]    Claire Windsor, Countess of Ulster         2   \n",
       "4     171           [1, 1]  Tom Campbell (California politician)         2   \n",
       "\n",
       "                                tokenized_paragraphs    word_splits  \\\n",
       "0  [[Samuel, Eli, Cornish, (, 1795, 6, November, ...   [0, 89, 407]   \n",
       "1  [[John, Renshaw, Starr, (, died, 1996, ), ,, w...   [0, 68, 164]   \n",
       "2  [[George, Reginald, Starr, DSO, MC, (, 6, Apri...    [0, 34, 85]   \n",
       "3  [[''Gloucester, family, banner, '', name, =mar...  [0, 264, 306]   \n",
       "4  [[Thomas, John, ``, Tom, '', Campbell, (, born...  [0, 161, 171]   \n",
       "\n",
       "      tst_word_splits tst_paragraph_splits  \n",
       "0            [0, 407]                  [5]  \n",
       "1       [0, 113, 164]               [1, 2]  \n",
       "2         [0, 73, 85]               [1, 2]  \n",
       "3  [0, 145, 204, 306]            [2, 1, 3]  \n",
       "4            [0, 171]                  [2]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our method is pretty good at not over segmenting biographies that have only one segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>paragraph_splits</th>\n",
       "      <th>person</th>\n",
       "      <th>segments</th>\n",
       "      <th>tokenized_paragraphs</th>\n",
       "      <th>word_splits</th>\n",
       "      <th>tst_word_splits</th>\n",
       "      <th>tst_paragraph_splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>153</td>\n",
       "      <td>[2]</td>\n",
       "      <td>Sima Lun</td>\n",
       "      <td>1</td>\n",
       "      <td>[[TitlesMarquess, of, Anle, Pavilion, Viscount...</td>\n",
       "      <td>[0, 153]</td>\n",
       "      <td>[0, 153, 153]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>203</td>\n",
       "      <td>[5]</td>\n",
       "      <td>Carlo Antonio Campioni</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Carlo, Antonio, Campioni, (, November, 16, ,...</td>\n",
       "      <td>[0, 203]</td>\n",
       "      <td>[0, 162, 203]</td>\n",
       "      <td>[3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>238</td>\n",
       "      <td>[11]</td>\n",
       "      <td>Charles Pinckney (South Carolina chief justice)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Charles, Pinckney, (, died, October, 29, ,, ...</td>\n",
       "      <td>[0, 238]</td>\n",
       "      <td>[0, 212, 217, 223, 229, 237, 238, 238]</td>\n",
       "      <td>[3, 1, 2, 1, 2, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>89</td>\n",
       "      <td>[2]</td>\n",
       "      <td>Eliza Lucas</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Eliza, Lucas, Pinckney, (, December, 28, ,, ...</td>\n",
       "      <td>[0, 89]</td>\n",
       "      <td>[0, 89]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>93</td>\n",
       "      <td>[2]</td>\n",
       "      <td>Dorothy Loudon</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Best, Leading, Actress, in, a, Musical, '', ...</td>\n",
       "      <td>[0, 93]</td>\n",
       "      <td>[0, 93]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>43</td>\n",
       "      <td>[1]</td>\n",
       "      <td>David Michelinie</td>\n",
       "      <td>1</td>\n",
       "      <td>[[David, Michelinie, (, born, May, 6, ,, 1948,...</td>\n",
       "      <td>[0, 43]</td>\n",
       "      <td>[0, 43]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>174</td>\n",
       "      <td>[9]</td>\n",
       "      <td>James L. Miller, Sr.</td>\n",
       "      <td>1</td>\n",
       "      <td>[[James, L., Miller, ,, Sr., (, 1897-1989, ), ...</td>\n",
       "      <td>[0, 174]</td>\n",
       "      <td>[0, 168, 174]</td>\n",
       "      <td>[4, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>195</td>\n",
       "      <td>[2]</td>\n",
       "      <td>Alton B. Parker</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Alton, Brooks, Parker, (, May, 14, ,, 1852, ...</td>\n",
       "      <td>[0, 195]</td>\n",
       "      <td>[0, 195]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>227</td>\n",
       "      <td>[3]</td>\n",
       "      <td>Mumtaz Mahal</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Mumtaz, Mahal, (, 1, September, 1593, –, 17,...</td>\n",
       "      <td>[0, 227]</td>\n",
       "      <td>[0, 227]</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>284</td>\n",
       "      <td>[9]</td>\n",
       "      <td>Larry Grantham</td>\n",
       "      <td>1</td>\n",
       "      <td>[[American, Football, League, All-AFL, All-Tim...</td>\n",
       "      <td>[0, 284]</td>\n",
       "      <td>[0, 15, 34, 284]</td>\n",
       "      <td>[2, 1, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    length paragraph_splits                                           person  \\\n",
       "5      153              [2]                                         Sima Lun   \n",
       "6      203              [5]                           Carlo Antonio Campioni   \n",
       "10     238             [11]  Charles Pinckney (South Carolina chief justice)   \n",
       "11      89              [2]                                      Eliza Lucas   \n",
       "13      93              [2]                                   Dorothy Loudon   \n",
       "15      43              [1]                                 David Michelinie   \n",
       "16     174              [9]                             James L. Miller, Sr.   \n",
       "25     195              [2]                                  Alton B. Parker   \n",
       "26     227              [3]                                     Mumtaz Mahal   \n",
       "28     284              [9]                                   Larry Grantham   \n",
       "\n",
       "    segments                               tokenized_paragraphs word_splits  \\\n",
       "5          1  [[TitlesMarquess, of, Anle, Pavilion, Viscount...    [0, 153]   \n",
       "6          1  [[Carlo, Antonio, Campioni, (, November, 16, ,...    [0, 203]   \n",
       "10         1  [[Charles, Pinckney, (, died, October, 29, ,, ...    [0, 238]   \n",
       "11         1  [[Eliza, Lucas, Pinckney, (, December, 28, ,, ...     [0, 89]   \n",
       "13         1  [[Best, Leading, Actress, in, a, Musical, '', ...     [0, 93]   \n",
       "15         1  [[David, Michelinie, (, born, May, 6, ,, 1948,...     [0, 43]   \n",
       "16         1  [[James, L., Miller, ,, Sr., (, 1897-1989, ), ...    [0, 174]   \n",
       "25         1  [[Alton, Brooks, Parker, (, May, 14, ,, 1852, ...    [0, 195]   \n",
       "26         1  [[Mumtaz, Mahal, (, 1, September, 1593, –, 17,...    [0, 227]   \n",
       "28         1  [[American, Football, League, All-AFL, All-Tim...    [0, 284]   \n",
       "\n",
       "                           tst_word_splits   tst_paragraph_splits  \n",
       "5                            [0, 153, 153]                 [1, 1]  \n",
       "6                            [0, 162, 203]                 [3, 2]  \n",
       "10  [0, 212, 217, 223, 229, 237, 238, 238]  [3, 1, 2, 1, 2, 1, 1]  \n",
       "11                                 [0, 89]                    [2]  \n",
       "13                                 [0, 93]                    [2]  \n",
       "15                                 [0, 43]                    [1]  \n",
       "16                           [0, 168, 174]                 [4, 5]  \n",
       "25                                [0, 195]                    [2]  \n",
       "26                                [0, 227]                    [3]  \n",
       "28                        [0, 15, 34, 284]              [2, 1, 6]  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios_data.loc[bios_data['segments'].isin([1])][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compare with Alexaner A Alemi and Paul Ginsparg's Method\n",
    "We took the code (https://github.com/alexalemi/segmentation.git) described in this article:\n",
    "http://arxiv.org/pdf/1503.05543v1.pdf  and modified it a little to fit our available embeddings DB and the presentation needs. Running it on the data gives pretty poort results, but can serve as basis for evaluation of our own method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('segmentation/code')\n",
    "from segmentation.code.segmentart import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 / 1000 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.segmentation import *\n",
    "def splits_list(bio,ind,acc):\n",
    "    if ind == len(bio)-1:\n",
    "        return acc\n",
    "    elif ind == 0:\n",
    "        acc.append(len(bio[ind][1].split()))\n",
    "    else:\n",
    "        acc.append(acc[ind-1]+len(bio[ind][1].split()))\n",
    "    return splits_list(bio,ind+1,acc)\n",
    "\n",
    "def indexlist2binary(index_list):\n",
    "    ret = \"1\"\n",
    "    for ordinal,split_location in enumerate(index_list):\n",
    "        if ordinal == 0:\n",
    "            continue\n",
    "        ret += \"0\"*(split_location - index_list[ordinal - 1])\n",
    "        ret += \"1\"\n",
    "    return ret\n",
    "\n",
    "alexmi = []\n",
    "ours = []\n",
    "for i in range(len(bios_data)):\n",
    "    onepiece = \" \".join([\" \".join(tp) for tp in bios_data.loc[i,'tokenized_paragraphs']])\n",
    "    gld = bios_data.loc[i,'word_splits']\n",
    "    tst = [0] + segmentize(onepiece,bios_data.loc[i,'segments'],vecs) + [bios_data.loc[i,'length']] if len(gld) > 2 else gld\n",
    "    if len(tst)*len(gld) > 0:\n",
    "        alexmi.append({\n",
    "            'person' : bios_data.loc[i,'person'], \n",
    "            'alexmi pk' : pk(indexlist2binary(gld),indexlist2binary(tst))\n",
    "        })\n",
    "        ours.append({\n",
    "                'person': bios_data.loc[i,'person'],\n",
    "                'our pk' : pk(indexlist2binary(gld),indexlist2binary(bios_data.loc[i,'tst_word_splits']))\n",
    "            })\n",
    "    progress(i, len(bios))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alexmi pk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.329642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.246120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.404736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.530077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.792393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         alexmi pk\n",
       "count  1000.000000\n",
       "mean      0.329642\n",
       "std       0.246120\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.404736\n",
       "75%       0.530077\n",
       "max       0.792393"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexmi = pandas.DataFrame(alexmi)\n",
    "ours = pandas.DataFrame(ours)\n",
    "alexmi.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>our pk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.200499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.163165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.204589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.309313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.830357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            our pk\n",
       "count  1000.000000\n",
       "mean      0.200499\n",
       "std       0.163165\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.204589\n",
       "75%       0.309313\n",
       "max       0.830357"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ours.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that even though we let the Alexmi code off all one-segment biographies, our method's mean is still better.\n",
    "It's probably not good enough for feeding to the segment classifier and getting good results.\n",
    "And there's a problem assessing segment classification where the number of segments is unequeal. Still,\n",
    "we can try to run the classifier on a few and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turn our segmentation into something the classifer can use\n",
    "def psplits2dformat(orig,df):\n",
    "    orig_format = list()\n",
    "    for i in range(len(df)):\n",
    "        if orig[i][0][0] != df.loc[i,'person']:\n",
    "            print(\"biography missmatch:\", orig[i][0][0] , df.loc[i,'person'])\n",
    "            return False\n",
    "        else:\n",
    "            bio = [(df.loc[i,'person'],[])]\n",
    "            added = 0\n",
    "            for pcount in df.loc[i,'tst_paragraph_splits']:\n",
    "                segment = \"\"\n",
    "                for j in range(pcount):\n",
    "                    segment += \" \".join(df.loc[i,'tokenized_paragraphs'][added])+\"\\n\"\n",
    "                    added += 1\n",
    "                bio[0][1].append(('?',segment))\n",
    "            orig_format.append(bio)\n",
    "    return orig_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "999 / 1000 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "pickleme = psplits2dformat(bios,bios_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Charles Pinckney (South Carolina chief justice)',\n",
       "  [('?',\n",
       "    \"Charles Pinckney ( died October 29 , 1758 ) was a noted South Carolina politician and colonial agent . He was also the father of two candidates for Vice-President and President . For four presidential elections in a row , from 1796 to 1808 , one of his sons would receive votes in the Electoral College .\\nPinckney was long prominent in colonial affairs , serving as attorney general of the Province of South Carolina in 1733 , speaker of the assembly in 1736 , 1738 and 1740 , chief justice of the province in 1752–1753 , and agent for South Carolina in England in 1753–1758 .\\nPinckney married Eliza Lucas as his second wife in 1744 . Three of their children lived to adulthood : Charles Cotesworth , a signer of the U.S. Constitution and the Federalist candidate for President in 1804 and 1808 and Vice-President in 1800 ; Harriott , who married Daniel Horry ; and Thomas , who negotiated Pinckney 's Treaty with Spain in 1795 and was the Federalist candidate for Vice-President in 1796 . Charles Pinckney was the uncle of Colonel Charles Pinckney ( 1731–1784 ) and the great-uncle of Governor Charles Pinckney ( 1757–1824 ) .\\n\"),\n",
       "   ('?', \"''This has been adapted from a 1911 encyclopedia . ''\\n\"),\n",
       "   ('?', 'DEFAULTSORT : Pinckney , Charles\\nC deaths\\n'),\n",
       "   ('?', 'Category : Pinckney family\\n'),\n",
       "   ('?',\n",
       "    'Category : South Carolina Attorneys General\\nCategory : Year of birth missing\\n'),\n",
       "   ('?', 'South Carolina-politician-stub\\n'),\n",
       "   ('?', '9akhrvq86petv4i35pp63pimlueitsj\\n')])]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickleme[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Charles Pinckney (South Carolina chief justice)',\n",
       "  [('Biography',\n",
       "    \"Charles Pinckney (died October 29, 1758) was a noted South Carolina politician and colonial agent. He was also the father of two candidates for Vice-President and President. For four presidential elections in a row, from 1796 to 1808, one of his sons would receive votes in the Electoral College. \\n\\nPinckney was long prominent in colonial affairs, serving as attorney general of the Province of South Carolina in 1733, speaker of the assembly in 1736, 1738 and 1740, chief justice of the province in 1752–1753, and agent for South Carolina in England in 1753–1758.\\n\\nPinckney married Eliza Lucas as his second wife in 1744.  Three of their children lived to adulthood: Charles Cotesworth, a signer of the U.S. Constitution and the Federalist candidate for President in 1804 and 1808 and Vice-President in 1800; Harriott, who married Daniel Horry; and Thomas, who negotiated Pinckney's Treaty with Spain in 1795 and was the Federalist candidate for Vice-President in 1796.  Charles Pinckney was the uncle of Colonel Charles Pinckney (1731–1784) and the great-uncle of Governor Charles Pinckney (1757–1824).\\n\\n''This has been adapted from a 1911 encyclopedia.''\\n\\nDEFAULTSORT:Pinckney, Charles\\nC deaths\\nCategory:Pinckney family\\nCategory:South Carolina Attorneys General\\nCategory:Year of birth missing\\n\\n\\nSouth Carolina-politician-stub\\n9akhrvq86petv4i35pp63pimlueitsj\\n\\n\\n\")])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"segmented18000.pkl\",\"wb\") as seg:\n",
    "    pickle.dump(pickleme,seg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
