{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle,sys,os,lda,scipy,pandas\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "bios = []\n",
    "for suf in ['18000']:\n",
    "    bios += pandas.read_pickle(\"train-corpus/corpus\"+suf+\".pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(i, end_val, bar_length=50):\n",
    "    percent = float(i) / end_val\n",
    "    hashes = '#' * int(round(percent * bar_length))\n",
    "    spaces = ' ' * (bar_length - len(hashes))\n",
    "    sys.stdout.write(\"\\r{0} / {1} Percent: [{2}] {3}%\".format(i, end_val, hashes + spaces, int(round(percent * 100))))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('John Renshaw Starr',\n",
       "  [('Summary',\n",
       "    \"John Renshaw Starr (died 1996), was one of two sons of Alfred Demarest Starr (an American) and Ethel Renshaw (English). He was a grandson of William Robert Renshaw. He was an artist and a soldier during the Second World War. His story is told in a book, ''The Starr Affair'', by Jean Overton Fuller.\\n\\n\"),\n",
       "   ('Release',\n",
       "    'By exploiting his ability to pass himself off as a Frenchman, he joined a group of French and Belgian prisoners who were released into the custody of the Red Cross and taken to Switzerland as the war in Europe drew to a close.\\n\\nStories from other SOE agents who shared his captivity at the Avenue Foch resulted in doubts being raised about his loyalty, and his case became the subject of an MI5 investigation, which concluded that although his behaviour was certainly suspicious, there were no grounds for criminal prosecution. \\n\\n')])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect paragraph data for every biography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 / 1000 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "bios_df = list()\n",
    "all_tokens = set()\n",
    "i = 0\n",
    "for bio in bios:\n",
    "    data = {\n",
    "        'person' : bio[0][0],\n",
    "        'tokenized_paragraphs' : list(),\n",
    "        'paragraph_splits' : list(),\n",
    "        'word_splits': [0],\n",
    "        'length' : 0,\n",
    "        'segments' : 0\n",
    "    }\n",
    "    number_of_words = 0\n",
    "    for segment in bio[0][1]:\n",
    "        number_of_paragraphs = 0\n",
    "        data['segments'] += 1\n",
    "        for paragraph_text in segment[1].split('\\n'):\n",
    "            tokens = word_tokenize(paragraph_text)\n",
    "            if len(tokens) > 0:\n",
    "                number_of_paragraphs += 1\n",
    "                number_of_words += len(tokens)\n",
    "                all_tokens |= set(tokens)\n",
    "                data['tokenized_paragraphs'].append(tokens)  \n",
    "        data['paragraph_splits'].append(number_of_paragraphs)\n",
    "        data['word_splits'].append(number_of_words)\n",
    "    data['length'] = number_of_words\n",
    "    bios_df.append(data)\n",
    "    i += 1\n",
    "    progress(i, len(bios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>paragraph_splits</th>\n",
       "      <th>person</th>\n",
       "      <th>segments</th>\n",
       "      <th>tokenized_paragraphs</th>\n",
       "      <th>word_splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>407</td>\n",
       "      <td>[1, 4]</td>\n",
       "      <td>Samuel Cornish</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Samuel, Eli, Cornish, (, 1795, 6, November, ...</td>\n",
       "      <td>[0, 89, 407]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>164</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>John Renshaw Starr</td>\n",
       "      <td>2</td>\n",
       "      <td>[[John, Renshaw, Starr, (, died, 1996, ), ,, w...</td>\n",
       "      <td>[0, 68, 164]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length paragraph_splits              person  segments  \\\n",
       "0     407           [1, 4]      Samuel Cornish         2   \n",
       "1     164           [1, 2]  John Renshaw Starr         2   \n",
       "\n",
       "                                tokenized_paragraphs   word_splits  \n",
       "0  [[Samuel, Eli, Cornish, (, 1795, 6, November, ...  [0, 89, 407]  \n",
       "1  [[John, Renshaw, Starr, (, died, 1996, ), ,, w...  [0, 68, 164]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios_data = pandas.DataFrame(bios_df)\n",
    "bios_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10329 / 10329 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "all_tokens_list = list(all_tokens)\n",
    "number_of_tokens = len(all_tokens_list)\n",
    "all_paragraphs = bios_data['tokenized_paragraphs'].sum()\n",
    "paragraphs_bow = np.zeros([len(all_paragraphs),number_of_tokens], dtype = np.int)\n",
    "tokens_indices_dict = dict()\n",
    "for i in range(number_of_tokens):\n",
    "    tokens_indices_dict[all_tokens_list[i]] = i\n",
    "    \n",
    "for i in range(len(all_paragraphs)):\n",
    "    for w in all_paragraphs[i]:\n",
    "        paragraphs_bow[i][tokens_indices_dict[w]] += 1\n",
    "        \n",
    "    progress(i + 1, len(all_paragraphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign LDA topics to paragraphs with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10329, 45763)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(all_tokens)\n",
    "paragraphs_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lda.lda.LDA at 0x7ff72163a518>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = lda.LDA(n_topics=20, n_iter=1500, random_state=1)\n",
    "model.fit(paragraphs_bow)  # model.fit_transform(X) is also available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with open('toy_topics.pkl','wb') as toy:\n",
    "#    pickle.dump(model,toy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = pandas.read_pickle('toy_topics.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate topic lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: the to . , his and a was of he in that for with had\n",
      "Topic 1: , and '' . of the in a his by with for 's ''The as\n",
      "Topic 2: the of , . for in and Award In ( ) at was has ''\n",
      "Topic 3: of , the . and was in his ) ( II Duke to son King\n",
      "Topic 4: '' , the . in and a as on of `` 's for film In\n",
      "Topic 5: the of and , . in his as to a is on by which also\n",
      "Topic 6: , of the . ) ( de '' was a in to and ; 's\n",
      "Topic 7: , ) ( was . and a born an is of He American known May\n",
      "Topic 8: the of , . in to and a as was from for on 's In\n",
      "Topic 9: , . and New of York '' with : by John `` ; * 's\n",
      "Topic 10: , the . to in and ( at first ) he He of his Open\n",
      "Topic 11: , . her in and she a was She of married had to 's on\n",
      "Topic 12: , in . the of and he was a at He to his University from\n",
      "Topic 13: , the in . and for a ( of ) on He with League was\n",
      "Topic 14: the , of . in and was War to a as 's Army during World\n",
      "Topic 15: , '' the . `` of to that a and is 's I it in\n",
      "Topic 16: the , . in a and on of was at 's is to The his\n",
      "Topic 17: '' : , `` ( ) Clan File - Wei CLN di Category Yi Yan\n",
      "Topic 18: '' ( ) * The : ''The `` [ ] ISBN as - London 's\n",
      "Topic 19: . : of ; = , # * 2 ] [ % I 3 (\n"
     ]
    }
   ],
   "source": [
    "topic_word = model.topic_word_ \n",
    "n_top_words = 20\n",
    "\n",
    "topics_words = list()\n",
    "\n",
    "word_freqs = dict()\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    # topic_words: words sorted by relevance to topic in descending order\n",
    "    topic_words = list(np.array(vocab)[np.argsort(topic_dist)[::-1]])#[:10]#[:-(n_top_words+1):-1]\n",
    "    #print(topic_words)\n",
    "    topics_words.append(topic_words)\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words[:15])))\n",
    "    for word in topic_words[:20]:\n",
    "        if word not in word_freqs:\n",
    "            word_freqs[word] = 1\n",
    "        else:\n",
    "            word_freqs[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vecs = pickle.load(open('/home/ilay/vecs.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def paragraph_to_vector(paragraph_tokens):\n",
    "    l = len(vecs['queen']) # len of the vector is 300\n",
    "    paragraph_accumulative = np.zeros(l)\n",
    "    topic_ratings = []\n",
    "    # just sum the paragraph words' vectors to get a semantic average of it\n",
    "    for ind,word in enumerate(paragraph_tokens):\n",
    "        if word in vecs:\n",
    "            paragraph_accumulative += vecs[word]\n",
    "    return paragraph_accumulative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each topic, make a representing vector by summing it's first 200 word-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_vectors = list()\n",
    "for topic_words in topics_words:\n",
    "    words_taken = 0\n",
    "    i = 0\n",
    "    vector = np.zeros(300)\n",
    "    while(words_taken < 200):\n",
    "        word = topic_words[i]\n",
    "        if (word not in word_freqs or word_freqs[word] < 5) and word in vecs:\n",
    "            vector += vecs[word]\n",
    "            words_taken += 1\n",
    "        i += 1\n",
    "    topic_vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make a list of topics for each paragraph by distance of topic vectors from the paragraph vector\n",
    "def paragraph_topics_rating(paragraph,topic_vectors):\n",
    "    cosine = scipy.spatial.distance.cosine\n",
    "    return np.argsort([cosine(paragraph_to_vector(paragraph),topic_vector) for i, topic_vector in enumerate(topic_vectors)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11,  7, 12, 16,  9,  3,  6,  4, 14, 13,  0,  1, 10, 19,  2, 15,  5,\n",
       "        8, 18, 17])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_topics_rating(bios_data.loc[1,'tokenized_paragraphs'][0],topic_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means topic 11 is most strongly linked to this pargraph, then topic 7, then 12 etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the text using according to the topic ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 / 1000 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "psplits = list()\n",
    "wsplits = list()\n",
    "for i in range(len(bios_data)):\n",
    "    last_paragraph_topics = np.array([])\n",
    "    number_of_paragraphs = 1\n",
    "    number_of_words = 0\n",
    "    psplit = list()\n",
    "    wsplit = list()\n",
    "    for tp in bios_data.loc[i,'tokenized_paragraphs']:\n",
    "        number_of_words += len(tp)\n",
    "        paragraph_topics = paragraph_topics_rating(tp,topic_vectors)[:3]\n",
    "        if len(last_paragraph_topics) > 0:\n",
    "            if len(np.intersect1d(paragraph_topics, last_paragraph_topics)) > 0:\n",
    "                number_of_paragraphs += 1\n",
    "            else:\n",
    "                psplit.append(number_of_paragraphs)\n",
    "                wsplit.append(number_of_words)\n",
    "                number_of_paragraphs = 1\n",
    "        else:\n",
    "            wsplit.append(0)\n",
    "        last_paragraph_topics = paragraph_topics\n",
    "        \n",
    "       \n",
    "    if number_of_paragraphs > 0:\n",
    "        psplit.append(number_of_paragraphs)\n",
    "        wsplit.append(number_of_words)\n",
    "    psplits.append(psplit)\n",
    "    wsplits.append(wsplit)\n",
    "    progress(i + 1, len(bios_data))\n",
    "bios_data['tst_word_splits'] = pandas.Series(wsplits,index=bios_data.index)\n",
    "bios_data['tst_paragraph_splits'] = pandas.Series(psplits, index=bios_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>paragraph_splits</th>\n",
       "      <th>person</th>\n",
       "      <th>segments</th>\n",
       "      <th>tokenized_paragraphs</th>\n",
       "      <th>word_splits</th>\n",
       "      <th>tst_word_splits</th>\n",
       "      <th>tst_paragraph_splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>407</td>\n",
       "      <td>[1, 4]</td>\n",
       "      <td>Samuel Cornish</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Samuel, Eli, Cornish, (, 1795, 6, November, ...</td>\n",
       "      <td>[0, 89, 407]</td>\n",
       "      <td>[0, 407]</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>164</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>John Renshaw Starr</td>\n",
       "      <td>2</td>\n",
       "      <td>[[John, Renshaw, Starr, (, died, 1996, ), ,, w...</td>\n",
       "      <td>[0, 68, 164]</td>\n",
       "      <td>[0, 113, 164]</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>George Reginald Starr</td>\n",
       "      <td>2</td>\n",
       "      <td>[[George, Reginald, Starr, DSO, MC, (, 6, Apri...</td>\n",
       "      <td>[0, 34, 85]</td>\n",
       "      <td>[0, 73, 85]</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>306</td>\n",
       "      <td>[5, 1]</td>\n",
       "      <td>Claire Windsor, Countess of Ulster</td>\n",
       "      <td>2</td>\n",
       "      <td>[[''Gloucester, family, banner, '', name, =mar...</td>\n",
       "      <td>[0, 264, 306]</td>\n",
       "      <td>[0, 145, 204, 306]</td>\n",
       "      <td>[2, 1, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>Tom Campbell (California politician)</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Thomas, John, ``, Tom, '', Campbell, (, born...</td>\n",
       "      <td>[0, 161, 171]</td>\n",
       "      <td>[0, 171]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length paragraph_splits                                person  segments  \\\n",
       "0     407           [1, 4]                        Samuel Cornish         2   \n",
       "1     164           [1, 2]                    John Renshaw Starr         2   \n",
       "2      85           [1, 2]                 George Reginald Starr         2   \n",
       "3     306           [5, 1]    Claire Windsor, Countess of Ulster         2   \n",
       "4     171           [1, 1]  Tom Campbell (California politician)         2   \n",
       "\n",
       "                                tokenized_paragraphs    word_splits  \\\n",
       "0  [[Samuel, Eli, Cornish, (, 1795, 6, November, ...   [0, 89, 407]   \n",
       "1  [[John, Renshaw, Starr, (, died, 1996, ), ,, w...   [0, 68, 164]   \n",
       "2  [[George, Reginald, Starr, DSO, MC, (, 6, Apri...    [0, 34, 85]   \n",
       "3  [[''Gloucester, family, banner, '', name, =mar...  [0, 264, 306]   \n",
       "4  [[Thomas, John, ``, Tom, '', Campbell, (, born...  [0, 161, 171]   \n",
       "\n",
       "      tst_word_splits tst_paragraph_splits  \n",
       "0            [0, 407]                  [5]  \n",
       "1       [0, 113, 164]               [1, 2]  \n",
       "2         [0, 73, 85]               [1, 2]  \n",
       "3  [0, 145, 204, 306]            [2, 1, 3]  \n",
       "4            [0, 171]                  [2]  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our method is pretty good at not over segmenting biographies that have only one segment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>paragraph_splits</th>\n",
       "      <th>person</th>\n",
       "      <th>segments</th>\n",
       "      <th>tokenized_paragraphs</th>\n",
       "      <th>word_splits</th>\n",
       "      <th>tst_word_splits</th>\n",
       "      <th>tst_paragraph_splits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>153</td>\n",
       "      <td>[2]</td>\n",
       "      <td>Sima Lun</td>\n",
       "      <td>1</td>\n",
       "      <td>[[TitlesMarquess, of, Anle, Pavilion, Viscount...</td>\n",
       "      <td>[0, 153]</td>\n",
       "      <td>[0, 153, 153]</td>\n",
       "      <td>[1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>203</td>\n",
       "      <td>[5]</td>\n",
       "      <td>Carlo Antonio Campioni</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Carlo, Antonio, Campioni, (, November, 16, ,...</td>\n",
       "      <td>[0, 203]</td>\n",
       "      <td>[0, 162, 203]</td>\n",
       "      <td>[3, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>238</td>\n",
       "      <td>[11]</td>\n",
       "      <td>Charles Pinckney (South Carolina chief justice)</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Charles, Pinckney, (, died, October, 29, ,, ...</td>\n",
       "      <td>[0, 238]</td>\n",
       "      <td>[0, 212, 217, 223, 229, 237, 238, 238]</td>\n",
       "      <td>[3, 1, 2, 1, 2, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>89</td>\n",
       "      <td>[2]</td>\n",
       "      <td>Eliza Lucas</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Eliza, Lucas, Pinckney, (, December, 28, ,, ...</td>\n",
       "      <td>[0, 89]</td>\n",
       "      <td>[0, 89]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>93</td>\n",
       "      <td>[2]</td>\n",
       "      <td>Dorothy Loudon</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Best, Leading, Actress, in, a, Musical, '', ...</td>\n",
       "      <td>[0, 93]</td>\n",
       "      <td>[0, 93]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>43</td>\n",
       "      <td>[1]</td>\n",
       "      <td>David Michelinie</td>\n",
       "      <td>1</td>\n",
       "      <td>[[David, Michelinie, (, born, May, 6, ,, 1948,...</td>\n",
       "      <td>[0, 43]</td>\n",
       "      <td>[0, 43]</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>174</td>\n",
       "      <td>[9]</td>\n",
       "      <td>James L. Miller, Sr.</td>\n",
       "      <td>1</td>\n",
       "      <td>[[James, L., Miller, ,, Sr., (, 1897-1989, ), ...</td>\n",
       "      <td>[0, 174]</td>\n",
       "      <td>[0, 168, 174]</td>\n",
       "      <td>[4, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>195</td>\n",
       "      <td>[2]</td>\n",
       "      <td>Alton B. Parker</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Alton, Brooks, Parker, (, May, 14, ,, 1852, ...</td>\n",
       "      <td>[0, 195]</td>\n",
       "      <td>[0, 195]</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>227</td>\n",
       "      <td>[3]</td>\n",
       "      <td>Mumtaz Mahal</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Mumtaz, Mahal, (, 1, September, 1593, –, 17,...</td>\n",
       "      <td>[0, 227]</td>\n",
       "      <td>[0, 227]</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>284</td>\n",
       "      <td>[9]</td>\n",
       "      <td>Larry Grantham</td>\n",
       "      <td>1</td>\n",
       "      <td>[[American, Football, League, All-AFL, All-Tim...</td>\n",
       "      <td>[0, 284]</td>\n",
       "      <td>[0, 15, 34, 284]</td>\n",
       "      <td>[2, 1, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    length paragraph_splits                                           person  \\\n",
       "5      153              [2]                                         Sima Lun   \n",
       "6      203              [5]                           Carlo Antonio Campioni   \n",
       "10     238             [11]  Charles Pinckney (South Carolina chief justice)   \n",
       "11      89              [2]                                      Eliza Lucas   \n",
       "13      93              [2]                                   Dorothy Loudon   \n",
       "15      43              [1]                                 David Michelinie   \n",
       "16     174              [9]                             James L. Miller, Sr.   \n",
       "25     195              [2]                                  Alton B. Parker   \n",
       "26     227              [3]                                     Mumtaz Mahal   \n",
       "28     284              [9]                                   Larry Grantham   \n",
       "\n",
       "    segments                               tokenized_paragraphs word_splits  \\\n",
       "5          1  [[TitlesMarquess, of, Anle, Pavilion, Viscount...    [0, 153]   \n",
       "6          1  [[Carlo, Antonio, Campioni, (, November, 16, ,...    [0, 203]   \n",
       "10         1  [[Charles, Pinckney, (, died, October, 29, ,, ...    [0, 238]   \n",
       "11         1  [[Eliza, Lucas, Pinckney, (, December, 28, ,, ...     [0, 89]   \n",
       "13         1  [[Best, Leading, Actress, in, a, Musical, '', ...     [0, 93]   \n",
       "15         1  [[David, Michelinie, (, born, May, 6, ,, 1948,...     [0, 43]   \n",
       "16         1  [[James, L., Miller, ,, Sr., (, 1897-1989, ), ...    [0, 174]   \n",
       "25         1  [[Alton, Brooks, Parker, (, May, 14, ,, 1852, ...    [0, 195]   \n",
       "26         1  [[Mumtaz, Mahal, (, 1, September, 1593, –, 17,...    [0, 227]   \n",
       "28         1  [[American, Football, League, All-AFL, All-Tim...    [0, 284]   \n",
       "\n",
       "                           tst_word_splits   tst_paragraph_splits  \n",
       "5                            [0, 153, 153]                 [1, 1]  \n",
       "6                            [0, 162, 203]                 [3, 2]  \n",
       "10  [0, 212, 217, 223, 229, 237, 238, 238]  [3, 1, 2, 1, 2, 1, 1]  \n",
       "11                                 [0, 89]                    [2]  \n",
       "13                                 [0, 93]                    [2]  \n",
       "15                                 [0, 43]                    [1]  \n",
       "16                           [0, 168, 174]                 [4, 5]  \n",
       "25                                [0, 195]                    [2]  \n",
       "26                                [0, 227]                    [3]  \n",
       "28                        [0, 15, 34, 284]              [2, 1, 6]  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios_data.loc[bios_data['segments'].isin([1])][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compare with Alexaner A Alemi and Paul Ginsparg's Method\n",
    "We took the code (https://github.com/alexalemi/segmentation.git) described in this article:\n",
    "http://arxiv.org/pdf/1503.05543v1.pdf  and modified it a little to fit our available embeddings DB and the presentation needs. Running it on the data gives pretty poort results, but can serve as basis for evaluation of our own method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.path.append('segmentation/code')\n",
    "from segmentation.code.segmentart import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 / 1000 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.segmentation import *\n",
    "def splits_list(bio,ind,acc):\n",
    "    if ind == len(bio)-1:\n",
    "        return acc\n",
    "    elif ind == 0:\n",
    "        acc.append(len(bio[ind][1].split()))\n",
    "    else:\n",
    "        acc.append(acc[ind-1]+len(bio[ind][1].split()))\n",
    "    return splits_list(bio,ind+1,acc)\n",
    "\n",
    "def indexlist2binary(index_list):\n",
    "    ret = \"1\"\n",
    "    for ordinal,split_location in enumerate(index_list):\n",
    "        if ordinal == 0:\n",
    "            continue\n",
    "        ret += \"0\"*(split_location - index_list[ordinal - 1])\n",
    "        ret += \"1\"\n",
    "    return ret\n",
    "\n",
    "alexmi = []\n",
    "ours = []\n",
    "for i in range(len(bios_data)):\n",
    "    onepiece = \" \".join([\" \".join(tp) for tp in bios_data.loc[i,'tokenized_paragraphs']])\n",
    "    gld = bios_data.loc[i,'word_splits']\n",
    "    tst = [0] + segmentize(onepiece,bios_data.loc[i,'segments'],vecs) + [bios_data.loc[i,'length']] if len(gld) > 2 else gld\n",
    "    if len(tst)*len(gld) > 0:\n",
    "        alexmi.append({\n",
    "            'person' : bios_data.loc[i,'person'], \n",
    "            'alexmi pk' : pk(indexlist2binary(gld),indexlist2binary(tst))\n",
    "        })\n",
    "        ours.append({\n",
    "                'person': bios_data.loc[i,'person'],\n",
    "                'our pk' : pk(indexlist2binary(gld),indexlist2binary(bios_data.loc[i,'tst_word_splits']))\n",
    "            })\n",
    "    progress(i, len(bios))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alexmi pk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.329642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.246120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.404736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.530077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.792393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         alexmi pk\n",
       "count  1000.000000\n",
       "mean      0.329642\n",
       "std       0.246120\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.404736\n",
       "75%       0.530077\n",
       "max       0.792393"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexmi = pandas.DataFrame(alexmi)\n",
    "ours = pandas.DataFrame(ours)\n",
    "alexmi.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>our pk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.200499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.163165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.204589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.309313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.830357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            our pk\n",
       "count  1000.000000\n",
       "mean      0.200499\n",
       "std       0.163165\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.204589\n",
       "75%       0.309313\n",
       "max       0.830357"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ours.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that even though we let the Alexmi code off all one-segment biographies, our method's mean is still better.\n",
    "It's probably not good enough for feeding to the segment classifier and getting good results.\n",
    "And there's a problem assessing segment classification where the number of segments is unequeal. Still,\n",
    "we can try to run the classifier on a few and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# turn our segmentation into something the classifer can use\n",
    "def psplits2dformat(orig,df):\n",
    "    orig_format = list()\n",
    "    for i in range(len(df)):\n",
    "        if orig[i][0][0] != df.loc[i,'person']:\n",
    "            print(\"biography missmatch:\", orig[i][0][0] , df.loc[i,'person'])\n",
    "            return False\n",
    "        else:\n",
    "            bio = [(df.loc[i,'person'],[])]\n",
    "            added = 0\n",
    "            for pcount in df.loc[i,'tst_paragraph_splits']:\n",
    "                segment = \"\"\n",
    "                for j in range(pcount):\n",
    "                    segment += \" \".join(df.loc[i,'tokenized_paragraphs'][added])+\"\\n\"\n",
    "                    added += 1\n",
    "                bio[0][1].append(('?',segment))\n",
    "            orig_format.append(bio)\n",
    "    return orig_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "999 / 1000 Percent: [##################################################] 100%"
     ]
    }
   ],
   "source": [
    "pickleme = psplits2dformat(bios,bios_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Charles Pinckney (South Carolina chief justice)',\n",
       "  [('?',\n",
       "    \"Charles Pinckney ( died October 29 , 1758 ) was a noted South Carolina politician and colonial agent . He was also the father of two candidates for Vice-President and President . For four presidential elections in a row , from 1796 to 1808 , one of his sons would receive votes in the Electoral College .\\nPinckney was long prominent in colonial affairs , serving as attorney general of the Province of South Carolina in 1733 , speaker of the assembly in 1736 , 1738 and 1740 , chief justice of the province in 1752–1753 , and agent for South Carolina in England in 1753–1758 .\\nPinckney married Eliza Lucas as his second wife in 1744 . Three of their children lived to adulthood : Charles Cotesworth , a signer of the U.S. Constitution and the Federalist candidate for President in 1804 and 1808 and Vice-President in 1800 ; Harriott , who married Daniel Horry ; and Thomas , who negotiated Pinckney 's Treaty with Spain in 1795 and was the Federalist candidate for Vice-President in 1796 . Charles Pinckney was the uncle of Colonel Charles Pinckney ( 1731–1784 ) and the great-uncle of Governor Charles Pinckney ( 1757–1824 ) .\\n\"),\n",
       "   ('?', \"''This has been adapted from a 1911 encyclopedia . ''\\n\"),\n",
       "   ('?', 'DEFAULTSORT : Pinckney , Charles\\nC deaths\\n'),\n",
       "   ('?', 'Category : Pinckney family\\n'),\n",
       "   ('?',\n",
       "    'Category : South Carolina Attorneys General\\nCategory : Year of birth missing\\n'),\n",
       "   ('?', 'South Carolina-politician-stub\\n'),\n",
       "   ('?', '9akhrvq86petv4i35pp63pimlueitsj\\n')])]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickleme[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Charles Pinckney (South Carolina chief justice)',\n",
       "  [('Biography',\n",
       "    \"Charles Pinckney (died October 29, 1758) was a noted South Carolina politician and colonial agent. He was also the father of two candidates for Vice-President and President. For four presidential elections in a row, from 1796 to 1808, one of his sons would receive votes in the Electoral College. \\n\\nPinckney was long prominent in colonial affairs, serving as attorney general of the Province of South Carolina in 1733, speaker of the assembly in 1736, 1738 and 1740, chief justice of the province in 1752–1753, and agent for South Carolina in England in 1753–1758.\\n\\nPinckney married Eliza Lucas as his second wife in 1744.  Three of their children lived to adulthood: Charles Cotesworth, a signer of the U.S. Constitution and the Federalist candidate for President in 1804 and 1808 and Vice-President in 1800; Harriott, who married Daniel Horry; and Thomas, who negotiated Pinckney's Treaty with Spain in 1795 and was the Federalist candidate for Vice-President in 1796.  Charles Pinckney was the uncle of Colonel Charles Pinckney (1731–1784) and the great-uncle of Governor Charles Pinckney (1757–1824).\\n\\n''This has been adapted from a 1911 encyclopedia.''\\n\\nDEFAULTSORT:Pinckney, Charles\\nC deaths\\nCategory:Pinckney family\\nCategory:South Carolina Attorneys General\\nCategory:Year of birth missing\\n\\n\\nSouth Carolina-politician-stub\\n9akhrvq86petv4i35pp63pimlueitsj\\n\\n\\n\")])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bios[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"segmented18000.pkl\",\"wb\") as seg:\n",
    "    pickle.dump(pickleme,seg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
